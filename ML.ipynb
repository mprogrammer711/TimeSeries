{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMM Date 1: 2023-12-20\n",
      "IMM Date 2: 2024-03-20\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "def get_third_wednesday(year, month):\n",
    "    \"\"\"Return the third Wednesday of the given month and year.\"\"\"\n",
    "    first_day = datetime.date(year, month, 1)\n",
    "    first_wednesday = first_day + datetime.timedelta(days=(2 - first_day.weekday() + 7) % 7)\n",
    "    third_wednesday = first_wednesday + datetime.timedelta(weeks=2)\n",
    "    return third_wednesday\n",
    "\n",
    "def calculate_imm_dates(date_str):\n",
    "    \"\"\"Calculate the next two IMM dates from a given date string 'yyyymmdd'.\"\"\"\n",
    "    date = datetime.datetime.strptime(date_str, '%Y%m%d').date()\n",
    "    imm_months = [3, 6, 9, 12]\n",
    "    \n",
    "    # Find the next IMM date\n",
    "    next_imm_date = None\n",
    "    for month in imm_months:\n",
    "        imm_date = get_third_wednesday(date.year, month)\n",
    "        if imm_date > date:\n",
    "            next_imm_date = imm_date\n",
    "            break\n",
    "    \n",
    "    # If no IMM date found in the current year, check the next year\n",
    "    if next_imm_date is None:\n",
    "        next_imm_date = get_third_wednesday(date.year + 1, imm_months[0])\n",
    "    \n",
    "    # Find the second IMM date\n",
    "    second_imm_date = None\n",
    "    for month in imm_months:\n",
    "        imm_date = get_third_wednesday(next_imm_date.year, month)\n",
    "        if imm_date > next_imm_date:\n",
    "            second_imm_date = imm_date\n",
    "            break\n",
    "    \n",
    "    # If no second IMM date found in the current year, check the next year\n",
    "    if second_imm_date is None:\n",
    "        second_imm_date = get_third_wednesday(next_imm_date.year + 1, imm_months[0])\n",
    "    \n",
    "    return next_imm_date, second_imm_date\n",
    "\n",
    "# Example usage\n",
    "date_str = '20231015'\n",
    "imm_date_1, imm_date_2 = calculate_imm_dates(date_str)\n",
    "print(f\"IMM Date 1: {imm_date_1}\")\n",
    "print(f\"IMM Date 2: {imm_date_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Define Attention Mechanism\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # hidden: (batch_size, hidden_size)\n",
    "        # encoder_outputs: (batch_size, seq_len, hidden_size)\n",
    "\n",
    "        seq_len = encoder_outputs.size(1)\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, seq_len, 1)  # (batch_size, seq_len, hidden_size)\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))  # (batch_size, seq_len, hidden_size)\n",
    "        energy = energy.transpose(1, 2)  # (batch_size, hidden_size, seq_len)\n",
    "        v = self.v.repeat(encoder_outputs.size(0), 1).unsqueeze(1)  # (batch_size, 1, hidden_size)\n",
    "        attention_weights = torch.bmm(v, energy).squeeze(1)  # (batch_size, seq_len)\n",
    "\n",
    "        return torch.softmax(attention_weights, dim=1)  # (batch_size, seq_len)\n",
    "\n",
    "\n",
    "# Define Encoder\n",
    "class LSTMEncoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout):\n",
    "        super(LSTMEncoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, input_size)\n",
    "        outputs, (hidden, cell) = self.lstm(x)\n",
    "        # outputs: (batch_size, seq_len, hidden_size)\n",
    "        return outputs, hidden, cell\n",
    "\n",
    "\n",
    "# Define Decoder with Attention\n",
    "class LSTMDecoderWithAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, num_layers, dropout):\n",
    "        super(LSTMDecoderWithAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.attention = Attention(hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size + 4, hidden_size, num_layers, batch_first=True, dropout=dropout)  # +4 for time features\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, hidden, cell, encoder_outputs, current_temporal_features):\n",
    "        # hidden: (num_layers, batch_size, hidden_size)\n",
    "        # encoder_outputs: (batch_size, seq_len, hidden_size)\n",
    "        # current_temporal_features: (batch_size, 1, 4)\n",
    "\n",
    "        attention_weights = self.attention(hidden[-1], encoder_outputs)  # (batch_size, seq_len)\n",
    "        attention_weights = attention_weights.unsqueeze(1)  # (batch_size, 1, seq_len)\n",
    "        context = torch.bmm(attention_weights, encoder_outputs)  # (batch_size, 1, hidden_size)\n",
    "\n",
    "        # Combine context with current temporal features\n",
    "        decoder_input = torch.cat([context, current_temporal_features], dim=2)  # (batch_size, 1, hidden_size + 4)\n",
    "        \n",
    "        # Pass through LSTM\n",
    "        output, (hidden, cell) = self.lstm(decoder_input, (hidden, cell))\n",
    "        \n",
    "        # Pass through the final fully connected layer\n",
    "        prediction = self.fc(output).squeeze(1)  # (batch_size, output_size)\n",
    "\n",
    "        return prediction, hidden, cell, attention_weights.squeeze(1)  # Return attention weights\n",
    "\n",
    "\n",
    "# Define full Encoder-Decoder Model\n",
    "class LSTMEncoderDecoderWithAttention(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, dropout):\n",
    "        super(LSTMEncoderDecoderWithAttention, self).__init__()\n",
    "        self.encoder = LSTMEncoder(input_size, hidden_size, num_layers, dropout)\n",
    "        self.decoder = LSTMDecoderWithAttention(hidden_size, output_size, num_layers, dropout)\n",
    "\n",
    "    def forward(self, encoder_input, current_temporal_features):\n",
    "        # encoder_input: (batch_size, seq_len, input_size)\n",
    "        # current_temporal_features: (batch_size, 1, 4)\n",
    "\n",
    "        # Encode input sequence\n",
    "        encoder_outputs, hidden, cell = self.encoder(encoder_input)\n",
    "\n",
    "        # Decode using the current temporal features and encoder outputs\n",
    "        prediction, _, _, attention_weights = self.decoder(hidden, cell, encoder_outputs, current_temporal_features)\n",
    "\n",
    "        return prediction, attention_weights\n",
    "\n",
    "\n",
    "# Load and preprocess data\n",
    "df = pd.read_csv('time_series_data.csv')\n",
    "\n",
    "# Check for missing values\n",
    "if df.isnull().sum().sum() > 0:\n",
    "    df = df.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "# Define your feature and target columns\n",
    "features = ['rate_level_1', 'rate_level_2',\n",
    "            'days_to_end_of_month', 'days_to_ECB_meeting', 'days_to_Fed_meeting', 'ois_sofr_rate']\n",
    "\n",
    "target = ['rate_level_1', 'rate_level_2']\n",
    "\n",
    "current_temporal_features = ['days_to_end_of_month', 'days_to_ECB_meeting', 'days_to_Fed_meeting', 'ois_sofr_rate']\n",
    "\n",
    "# Normalize data\n",
    "scaler_features = MinMaxScaler()\n",
    "scaler_target = MinMaxScaler()\n",
    "\n",
    "# Normalize feature columns\n",
    "df[features] = scaler_features.fit_transform(df[features])\n",
    "\n",
    "# Normalize target columns\n",
    "df[target] = scaler_target.fit_transform(df[target])\n",
    "\n",
    "# Function to create sequences\n",
    "def create_sequences(data, target_data, n_timesteps):\n",
    "    X, y, current_time_features = [], [], []\n",
    "    for i in range(len(data) - n_timesteps):\n",
    "        X.append(data[i:i + n_timesteps].values)\n",
    "        y.append(target_data.iloc[i + n_timesteps].values)\n",
    "        current_time_features.append(data[current_temporal_features].iloc[i + n_timesteps].values)\n",
    "    return np.array(X), np.array(y), np.array(current_time_features)\n",
    "\n",
    "# Prepare sequences\n",
    "n_timesteps_input = 12\n",
    "X, y, current_temporal = create_sequences(df[features], df[target], n_timesteps_input)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test, current_temporal_train, current_temporal_test = train_test_split(\n",
    "    X, y, current_temporal, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "current_temporal_train = torch.tensor(current_temporal_train, dtype=torch.float32).unsqueeze(1)\n",
    "current_temporal_test = torch.tensor(current_temporal_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = len(features)  # 6 features\n",
    "hidden_size = 128\n",
    "output_size = len(target)  # 2 targets\n",
    "num_layers = 2\n",
    "dropout = 0.3\n",
    "learning_rate = 0.001\n",
    "n_epochs = 50\n",
    "\n",
    "# Initialize model\n",
    "model = LSTMEncoderDecoderWithAttention(input_size, hidden_size, output_size, num_layers, dropout)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    output, _ = model(X_train, current_temporal_train)\n",
    "    \n",
    "    # Calculate loss\n",
    "    loss = criterion(output, y_train)\n",
    "    \n",
    "    # Backward pass and optimize\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{n_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Example evaluation on test set\n",
    "model.eval()\n",
    "attention_weights_list = []\n",
    "with torch.no_grad():\n",
    "    forecast, attention_weights = model(X_test, current_temporal_test)\n",
    "    attention_weights_list.append(attention_weights.cpu().numpy())\n",
    "\n",
    "# Convert list to numpy array\n",
    "attention_weights_array = np.concatenate(attention_weights_list, axis=0)\n",
    "\n",
    "# Inverse-transform the predictions to the original scale\n",
    "forecast_original_scale = scaler_target.inverse_transform(forecast.cpu().numpy())\n",
    "y_test_original_scale = scaler_target.inverse_transform(y_test.cpu().numpy())\n",
    "\n",
    "# Compare predictions to the actual values\n",
    "print(\"Predictions on original scale:\", forecast_original_scale)\n",
    "print(\"True values on original scale:\", y_test_original_scale)\n",
    "\n",
    "# Visualize attention weights for a specific sample\n",
    "sample_index = 0  # Change this to visualize different samples\n",
    "attention_weights_sample = attention_weights_array[sample_index]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(attention_weights_sample.reshape(1, -1), cmap='viridis', annot=True)\n",
    "plt.title('Attention Weights for Sample Index {}'.format(sample_index))\n",
    "plt.xlabel('Input Sequence Index')\n",
    "plt.ylabel('Attention Weight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, encoding_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for the autoencoder\n",
    "input_dim = len(features)\n",
    "encoding_dim = 3  # Reduced dimension\n",
    "\n",
    "# Initialize the autoencoder\n",
    "autoencoder = Autoencoder(input_dim, encoding_dim)\n",
    "ae_optimizer = optim.Adam(autoencoder.parameters(), lr=learning_rate)\n",
    "ae_criterion = nn.MSELoss()\n",
    "\n",
    "# Prepare data for autoencoder\n",
    "X_autoencoder = torch.tensor(df[features].values, dtype=torch.float32)\n",
    "\n",
    "# Training loop for autoencoder\n",
    "n_epochs_ae = 100\n",
    "for epoch in range(n_epochs_ae):\n",
    "    autoencoder.train()\n",
    "    ae_optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    _, decoded = autoencoder(X_autoencoder)\n",
    "    \n",
    "    # Calculate loss\n",
    "    loss = ae_criterion(decoded, X_autoencoder)\n",
    "    \n",
    "    # Backward pass and optimize\n",
    "    loss.backward()\n",
    "    ae_optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Autoencoder Epoch [{epoch+1}/{n_epochs_ae}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the input features using the trained autoencoder\n",
    "autoencoder.eval()\n",
    "with torch.no_grad():\n",
    "    encoded_features, _ = autoencoder(X_autoencoder)\n",
    "    df_encoded = pd.DataFrame(encoded_features.numpy(), columns=[f'encoded_{i}' for i in range(encoding_dim)])\n",
    "\n",
    "# Update the feature columns to use the encoded features\n",
    "encoded_feature_columns = [f'encoded_{i}' for i in range(encoding_dim)]\n",
    "df[encoded_feature_columns] = df_encoded\n",
    "\n",
    "# Function to create sequences with encoded features\n",
    "def create_sequences(data, target_data, n_timesteps):\n",
    "    X, y, current_time_features = [], [], []\n",
    "    for i in range(len(data) - n_timesteps):\n",
    "        X.append(data[encoded_feature_columns].iloc[i:i + n_timesteps].values)\n",
    "        y.append(target_data.iloc[i + n_timesteps].values)\n",
    "        current_time_features.append(data[current_temporal_features].iloc[i + n_timesteps].values)\n",
    "    return np.array(X), np.array(y), np.array(current_time_features)\n",
    "\n",
    "# Prepare sequences\n",
    "n_timesteps_input = 12\n",
    "X, y, current_temporal = create_sequences(df, df[target], n_timesteps_input)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test, current_temporal_train, current_temporal_test = train_test_split(\n",
    "    X, y, current_temporal, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "current_temporal_train = torch.tensor(current_temporal_train, dtype=torch.float32).unsqueeze(1)\n",
    "current_temporal_test = torch.tensor(current_temporal_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Hyperparameters for the LSTM model\n",
    "input_size = encoding_dim  # Reduced dimension\n",
    "hidden_size = 128\n",
    "output_size = len(target)  # 2 targets\n",
    "num_layers = 2\n",
    "dropout = 0.3\n",
    "learning_rate = 0.001\n",
    "n_epochs = 50\n",
    "\n",
    "# Initialize the LSTM model\n",
    "model = LSTMEncoderDecoderWithAttention(input_size, hidden_size, output_size, num_layers, dropout)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training loop for the LSTM model\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    output, _ = model(X_train, current_temporal_train)\n",
    "    \n",
    "    # Calculate loss\n",
    "    loss = criterion(output, y_train)\n",
    "    \n",
    "    # Backward pass and optimize\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{n_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Example evaluation on test set\n",
    "model.eval()\n",
    "attention_weights_list = []\n",
    "with torch.no_grad():\n",
    "    forecast, attention_weights = model(X_test, current_temporal_test)\n",
    "    attention_weights_list.append(attention_weights.cpu().numpy())\n",
    "\n",
    "# Convert list to numpy array\n",
    "attention_weights_array = np.concatenate(attention_weights_list, axis=0)\n",
    "\n",
    "# Inverse-transform the predictions to the original scale\n",
    "forecast_original_scale = scaler_target.inverse_transform(forecast.cpu().numpy())\n",
    "y_test_original_scale = scaler_target.inverse_transform(y_test.cpu().numpy())\n",
    "\n",
    "# Compare predictions to the actual values\n",
    "print(\"Predictions on original scale:\", forecast_original_scale)\n",
    "print(\"True values on original scale:\", y_test_original_scale)\n",
    "\n",
    "# Visualize attention weights for a specific sample\n",
    "sample_index = 0  # Change this to visualize different samples\n",
    "attention_weights_sample = attention_weights_array[sample_index]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(attention_weights_sample.reshape(1, -1), cmap='viridis', annot=True)\n",
    "plt.title('Attention Weights for Sample Index {}'.format(sample_index))\n",
    "plt.xlabel('Input Sequence Index')\n",
    "plt.ylabel('Attention Weight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract encoded features using the trained autoencoder\n",
    "autoencoder.eval()\n",
    "with torch.no_grad():\n",
    "    encoded_features, _ = autoencoder(X_autoencoder)\n",
    "    df_encoded = pd.DataFrame(encoded_features.numpy(), columns=[f'encoded_{i}' for i in range(encoding_dim)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Apply t-SNE to reduce the dimensionality of the encoded features to 2D\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "encoded_features_2d = tsne.fit_transform(df_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the 2D encoded features\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(encoded_features_2d[:, 0], encoded_features_2d[:, 1], c='blue', alpha=0.5)\n",
    "plt.title('2D Visualization of Encoded Features using t-SNE')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Define the Autoencoder\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, encoding_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n",
    "\n",
    "# Load and preprocess data\n",
    "df = pd.read_csv('time_series_data.csv')\n",
    "\n",
    "# Check for missing values\n",
    "if df.isnull().sum().sum() > 0:\n",
    "    df = df.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "# Define your feature and target columns\n",
    "features = ['rate_level_1', 'rate_level_2',\n",
    "            'days_to_end_of_month', 'days_to_ECB_meeting', 'days_to_Fed_meeting', 'ois_sofr_rate']\n",
    "\n",
    "target = ['rate_level_1', 'rate_level_2']\n",
    "\n",
    "current_temporal_features = ['days_to_end_of_month', 'days_to_ECB_meeting', 'days_to_Fed_meeting', 'ois_sofr_rate']\n",
    "\n",
    "# Normalize data\n",
    "scaler_features = MinMaxScaler()\n",
    "scaler_target = MinMaxScaler()\n",
    "\n",
    "# Normalize feature columns\n",
    "df[features] = scaler_features.fit_transform(df[features])\n",
    "\n",
    "# Normalize target columns\n",
    "df[target] = scaler_target.fit_transform(df[target])\n",
    "\n",
    "# Hyperparameters for the autoencoder\n",
    "input_dim = len(features)\n",
    "encoding_dim = 3  # Reduced dimension\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Initialize the autoencoder\n",
    "autoencoder = Autoencoder(input_dim, encoding_dim)\n",
    "ae_optimizer = optim.Adam(autoencoder.parameters(), lr=learning_rate)\n",
    "ae_criterion = nn.MSELoss()\n",
    "\n",
    "# Prepare data for autoencoder\n",
    "X_autoencoder = torch.tensor(df[features].values, dtype=torch.float32)\n",
    "\n",
    "# Training loop for autoencoder\n",
    "n_epochs_ae = 100\n",
    "for epoch in range(n_epochs_ae):\n",
    "    autoencoder.train()\n",
    "    ae_optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    _, decoded = autoencoder(X_autoencoder)\n",
    "    \n",
    "    # Calculate loss\n",
    "    loss = ae_criterion(decoded, X_autoencoder)\n",
    "    \n",
    "    # Backward pass and optimize\n",
    "    loss.backward()\n",
    "    ae_optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Autoencoder Epoch [{epoch+1}/{n_epochs_ae}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Extract encoded features using the trained autoencoder\n",
    "autoencoder.eval()\n",
    "with torch.no_grad():\n",
    "    encoded_features, _ = autoencoder(X_autoencoder)\n",
    "    df_encoded = pd.DataFrame(encoded_features.numpy(), columns=[f'encoded_{i}' for i in range(encoding_dim)])\n",
    "\n",
    "# Apply t-SNE to reduce the dimensionality of the encoded features to 2D\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "encoded_features_2d = tsne.fit_transform(df_encoded)\n",
    "\n",
    "# Visualize the 2D encoded features\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(encoded_features_2d[:, 0], encoded_features_2d[:, 1], c='blue', alpha=0.5)\n",
    "plt.title('2D Visualization of Encoded Features using t-SNE')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Assuming you have a trained model and test data\n",
    "result = permutation_importance(model, X_test, y_test, n_repeats=10, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Get feature importance scores\n",
    "feature_importance = result.importances_mean\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(len(feature_importance)), feature_importance, align='center')\n",
    "plt.yticks(range(len(feature_importance)), encoded_feature_columns)\n",
    "plt.xlabel('Permutation Feature Importance')\n",
    "plt.title('Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Train a Random Forest model\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importance scores\n",
    "feature_importance = rf_model.feature_importances_\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(len(feature_importance)), feature_importance, align='center')\n",
    "plt.yticks(range(len(feature_importance)), encoded_feature_columns)\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Feature Importance from Random Forest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = df[features].corr()\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot pairplot\n",
    "sns.pairplot(df[features])\n",
    "plt.suptitle('Pairplot of Features', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cluster heatmap\n",
    "sns.clustermap(df[features].corr(), annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Cluster Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature distribution\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, feature in enumerate(features):\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    sns.histplot(df[feature], kde=True)\n",
    "    plt.title(f'Distribution of {feature}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot boxplot\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.boxplot(data=df[features])\n",
    "plt.title('Boxplot of Features')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df_encoded contains the encoded features\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(df_encoded, cmap='viridis', cbar=True)\n",
    "plt.title('Heatmap of Encoded Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "# Select top k features based on chi-square test\n",
    "k = 5\n",
    "selector = SelectKBest(chi2, k=k)\n",
    "X_new = selector.fit_transform(X, y)\n",
    "selected_features = X.columns[selector.get_support()]\n",
    "print(\"Selected features:\", selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Initialize the model\n",
    "model = RandomForestRegressor()\n",
    "\n",
    "# Initialize RFE with the model\n",
    "rfe = RFE(model, n_features_to_select=5)\n",
    "X_rfe = rfe.fit_transform(X, y)\n",
    "selected_features = X.columns[rfe.support_]\n",
    "print(\"Selected features:\", selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Initialize Lasso with a regularization parameter\n",
    "lasso = Lasso(alpha=0.01)\n",
    "lasso.fit(X, y)\n",
    "\n",
    "# Get the coefficients\n",
    "lasso_coefficients = lasso.coef_\n",
    "\n",
    "# Select features with non-zero coefficients\n",
    "selected_features = X.columns[lasso_coefficients != 0]\n",
    "print(\"Selected features:\", selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Train a Random Forest model\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Get feature importance scores\n",
    "feature_importance = model.feature_importances_\n",
    "\n",
    "# Select top k features\n",
    "k = 5\n",
    "indices = np.argsort(feature_importance)[-k:]\n",
    "selected_features = X.columns[indices]\n",
    "print(\"Selected features:\", selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "# Calculate mutual information scores\n",
    "mi_scores = mutual_info_regression(X, y)\n",
    "\n",
    "# Select top k features\n",
    "k = 5\n",
    "indices = np.argsort(mi_scores)[-k:]\n",
    "selected_features = X.columns[indices]\n",
    "print(\"Selected features:\", selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load and preprocess data\n",
    "df = pd.read_csv('time_series_data.csv')\n",
    "\n",
    "# Check for missing values\n",
    "if df.isnull().sum().sum() > 0:\n",
    "    df = df.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "# Define your feature and target columns\n",
    "features = ['rate_level_1', 'rate_level_2',\n",
    "            'days_to_end_of_month', 'days_to_ECB_meeting', 'days_to_Fed_meeting', 'ois_sofr_rate']\n",
    "\n",
    "target = ['rate_level_1', 'rate_level_2']\n",
    "\n",
    "# Normalize data\n",
    "scaler_features = MinMaxScaler()\n",
    "scaler_target = MinMaxScaler()\n",
    "\n",
    "# Normalize feature columns\n",
    "df[features] = scaler_features.fit_transform(df[features])\n",
    "\n",
    "# Normalize target columns\n",
    "df[target] = scaler_target.fit_transform(df[target])\n",
    "\n",
    "# Function to create sequences\n",
    "def create_sequences(data, target_data, n_timesteps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - n_timesteps):\n",
    "        X.append(data[i:i + n_timesteps].values.flatten())\n",
    "        y.append(target_data.iloc[i + n_timesteps].values)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Prepare sequences\n",
    "n_timesteps_input = 12\n",
    "X, y = create_sequences(df[features], df[target], n_timesteps_input)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize XGBoost model\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = xgb_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Mean Squared Error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse:.4f}')\n",
    "\n",
    "# Inverse-transform the predictions to the original scale\n",
    "y_pred_original_scale = scaler_target.inverse_transform(y_pred.reshape(-1, len(target)))\n",
    "y_test_original_scale = scaler_target.inverse_transform(y_test)\n",
    "\n",
    "# Compare predictions to the actual values\n",
    "print(\"Predictions on original scale:\", y_pred_original_scale)\n",
    "print(\"True values on original scale:\", y_test_original_scale)\n",
    "\n",
    "# Visualize predictions vs actual values\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(y_test_original_scale[:, 0], label='True Rate Level 1')\n",
    "plt.plot(y_pred_original_scale[:, 0], label='Predicted Rate Level 1')\n",
    "plt.legend()\n",
    "plt.title('True vs Predicted Rate Level 1')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(y_test_original_scale[:, 1], label='True Rate Level 2')\n",
    "plt.plot(y_pred_original_scale[:, 1], label='Predicted Rate Level 2')\n",
    "plt.legend()\n",
    "plt.title('True vs Predicted Rate Level 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# Initialize the SHAP explainer\n",
    "explainer = shap.Explainer(model, X_train)\n",
    "\n",
    "# Calculate SHAP values for the test set\n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "# Plot SHAP summary plot\n",
    "shap.summary_plot(shap_values, X_test, feature_names=encoded_feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Define Attention Mechanism\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # hidden: (batch_size, hidden_size)\n",
    "        # encoder_outputs: (batch_size, seq_len, hidden_size)\n",
    "\n",
    "        seq_len = encoder_outputs.size(1)\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, seq_len, 1)  # (batch_size, seq_len, hidden_size)\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))  # (batch_size, seq_len, hidden_size)\n",
    "        energy = energy.transpose(1, 2)  # (batch_size, hidden_size, seq_len)\n",
    "        v = self.v.repeat(encoder_outputs.size(0), 1).unsqueeze(1)  # (batch_size, 1, hidden_size)\n",
    "        attention_weights = torch.bmm(v, energy).squeeze(1)  # (batch_size, seq_len)\n",
    "\n",
    "        return torch.softmax(attention_weights, dim=1)  # (batch_size, seq_len)\n",
    "\n",
    "\n",
    "# Define Encoder\n",
    "class LSTMEncoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout):\n",
    "        super(LSTMEncoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, input_size)\n",
    "        outputs, (hidden, cell) = self.lstm(x)\n",
    "        # outputs: (batch_size, seq_len, hidden_size)\n",
    "        return outputs, hidden, cell\n",
    "\n",
    "\n",
    "# Define Decoder with Attention\n",
    "class LSTMDecoderWithAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, num_layers, dropout):\n",
    "        super(LSTMDecoderWithAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.attention = Attention(hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size + 4, hidden_size, num_layers, batch_first=True, dropout=dropout)  # +4 for time features\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, hidden, cell, encoder_outputs, current_temporal_features):\n",
    "        # hidden: (num_layers, batch_size, hidden_size)\n",
    "        # encoder_outputs: (batch_size, seq_len, hidden_size)\n",
    "        # current_temporal_features: (batch_size, 1, 4)\n",
    "\n",
    "        attention_weights = self.attention(hidden[-1], encoder_outputs)  # (batch_size, seq_len)\n",
    "        attention_weights = attention_weights.unsqueeze(1)  # (batch_size, 1, seq_len)\n",
    "        context = torch.bmm(attention_weights, encoder_outputs)  # (batch_size, 1, hidden_size)\n",
    "\n",
    "        # Combine context with current temporal features\n",
    "        decoder_input = torch.cat([context, current_temporal_features], dim=2)  # (batch_size, 1, hidden_size + 4)\n",
    "        \n",
    "        # Pass through LSTM\n",
    "        output, (hidden, cell) = self.lstm(decoder_input, (hidden, cell))\n",
    "        \n",
    "        # Pass through the final fully connected layer\n",
    "        prediction = self.fc(output).squeeze(1)  # (batch_size, output_size)\n",
    "\n",
    "        return prediction, hidden, cell, attention_weights.squeeze(1)  # Return attention weights\n",
    "\n",
    "\n",
    "# Define full Encoder-Decoder Model\n",
    "class LSTMEncoderDecoderWithAttention(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, dropout):\n",
    "        super(LSTMEncoderDecoderWithAttention, self).__init__()\n",
    "        self.encoder = LSTMEncoder(input_size, hidden_size, num_layers, dropout)\n",
    "        self.decoder = LSTMDecoderWithAttention(hidden_size, output_size, num_layers, dropout)\n",
    "\n",
    "    def forward(self, encoder_input, current_temporal_features):\n",
    "        # encoder_input: (batch_size, seq_len, input_size)\n",
    "        # current_temporal_features: (batch_size, 1, 4)\n",
    "\n",
    "        # Encode input sequence\n",
    "        encoder_outputs, hidden, cell = self.encoder(encoder_input)\n",
    "\n",
    "        # Decode using the current temporal features and encoder outputs\n",
    "        prediction, _, _, attention_weights = self.decoder(hidden, cell, encoder_outputs, current_temporal_features)\n",
    "\n",
    "        return prediction, attention_weights\n",
    "\n",
    "\n",
    "# Load and preprocess data\n",
    "df = pd.read_csv('time_series_data.csv')\n",
    "\n",
    "# Check for missing values\n",
    "if df.isnull().sum().sum() > 0:\n",
    "    df = df.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "# Define your feature and target columns\n",
    "features = ['rate_level_1', 'rate_level_2',\n",
    "            'days_to_end_of_month', 'days_to_ECB_meeting', 'days_to_Fed_meeting', 'ois_sofr_rate']\n",
    "\n",
    "target = ['rate_level_1', 'rate_level_2']\n",
    "\n",
    "current_temporal_features = ['days_to_end_of_month', 'days_to_ECB_meeting', 'days_to_Fed_meeting', 'ois_sofr_rate']\n",
    "\n",
    "# Normalize data\n",
    "scaler_features = MinMaxScaler()\n",
    "scaler_target = MinMaxScaler()\n",
    "\n",
    "# Normalize feature columns\n",
    "df[features] = scaler_features.fit_transform(df[features])\n",
    "\n",
    "# Normalize target columns\n",
    "df[target] = scaler_target.fit_transform(df[target])\n",
    "\n",
    "# Function to create sequences\n",
    "def create_sequences(data, target_data, n_timesteps):\n",
    "    X, y, current_time_features = [], [], []\n",
    "    for i in range(len(data) - n_timesteps):\n",
    "        X.append(data[i:i + n_timesteps].values)\n",
    "        y.append(target_data.iloc[i + n_timesteps].values)\n",
    "        current_time_features.append(data[current_temporal_features].iloc[i + n_timesteps].values)\n",
    "    return np.array(X), np.array(y), np.array(current_time_features)\n",
    "\n",
    "# Prepare sequences\n",
    "n_timesteps_input = 12\n",
    "X, y, current_temporal = create_sequences(df[features], df[target], n_timesteps_input)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test, current_temporal_train, current_temporal_test = train_test_split(\n",
    "    X, y, current_temporal, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "current_temporal_train = torch.tensor(current_temporal_train, dtype=torch.float32).unsqueeze(1)\n",
    "current_temporal_test = torch.tensor(current_temporal_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = len(features)  # 6 features\n",
    "hidden_size = 128\n",
    "output_size = len(target)  # 2 targets\n",
    "num_layers = 2\n",
    "dropout = 0.3\n",
    "learning_rate = 0.001\n",
    "n_epochs = 50\n",
    "\n",
    "# Initialize model\n",
    "model = LSTMEncoderDecoderWithAttention(input_size, hidden_size, output_size, num_layers, dropout)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    output, _ = model(X_train, current_temporal_train)\n",
    "    \n",
    "    # Calculate loss\n",
    "    loss = criterion(output, y_train)\n",
    "    \n",
    "    # Backward pass and optimize\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{n_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Example evaluation on test set\n",
    "model.eval()\n",
    "attention_weights_list = []\n",
    "with torch.no_grad():\n",
    "    forecast, attention_weights = model(X_test, current_temporal_test)\n",
    "    attention_weights_list.append(attention_weights.cpu().numpy())\n",
    "\n",
    "# Convert list to numpy array\n",
    "attention_weights_array = np.concatenate(attention_weights_list, axis=0)\n",
    "\n",
    "# Inverse-transform the predictions to the original scale\n",
    "forecast_original_scale = scaler_target.inverse_transform(forecast.cpu().numpy())\n",
    "y_test_original_scale = scaler_target.inverse_transform(y_test.cpu().numpy())\n",
    "\n",
    "# Compare predictions to the actual values\n",
    "print(\"Predictions on original scale:\", forecast_original_scale)\n",
    "print(\"True values on original scale:\", y_test_original_scale)\n",
    "\n",
    "# Visualize attention weights for a specific sample\n",
    "sample_index = 0  # Change this to visualize different samples\n",
    "attention_weights_sample = attention_weights_array[sample_index]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(attention_weights_sample.reshape(1, -1), cmap='viridis', annot=True)\n",
    "plt.title('Attention Weights for Sample Index {}'.format(sample_index))\n",
    "plt.xlabel('Input Sequence Index')\n",
    "plt.ylabel('Attention Weight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, target_data, n_timesteps, n_future):\n",
    "    X, y, current_time_features = [], [], []\n",
    "    for i in range(len(data) - n_timesteps - n_future + 1):\n",
    "        X.append(data[i:i + n_timesteps].values)\n",
    "        y.append(target_data.iloc[i + n_timesteps:i + n_timesteps + n_future].values)\n",
    "        current_time_features.append(data[current_temporal_features].iloc[i + n_timesteps:i + n_timesteps + n_future].values)\n",
    "    return np.array(X), np.array(y), np.array(current_time_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMDecoderWithAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, num_layers, dropout, n_future):\n",
    "        super(LSTMDecoderWithAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.n_future = n_future\n",
    "\n",
    "        self.attention = Attention(hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size + 4, hidden_size, num_layers, batch_first=True, dropout=dropout)  # +4 for time features\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, hidden, cell, encoder_outputs, current_temporal_features):\n",
    "        # hidden: (num_layers, batch_size, hidden_size)\n",
    "        # encoder_outputs: (batch_size, seq_len, hidden_size)\n",
    "        # current_temporal_features: (batch_size, n_future, 4)\n",
    "\n",
    "        predictions = []\n",
    "        attention_weights_list = []\n",
    "\n",
    "        for t in range(self.n_future):\n",
    "            attention_weights = self.attention(hidden[-1], encoder_outputs)  # (batch_size, seq_len)\n",
    "            attention_weights = attention_weights.unsqueeze(1)  # (batch_size, 1, seq_len)\n",
    "            context = torch.bmm(attention_weights, encoder_outputs)  # (batch_size, 1, hidden_size)\n",
    "\n",
    "            # Combine context with current temporal features\n",
    "            decoder_input = torch.cat([context, current_temporal_features[:, t:t+1, :]], dim=2)  # (batch_size, 1, hidden_size + 4)\n",
    "            \n",
    "            # Pass through LSTM\n",
    "            output, (hidden, cell) = self.lstm(decoder_input, (hidden, cell))\n",
    "            \n",
    "            # Pass through the final fully connected layer\n",
    "            prediction = self.fc(output).squeeze(1)  # (batch_size, output_size)\n",
    "            predictions.append(prediction)\n",
    "            attention_weights_list.append(attention_weights.squeeze(1))\n",
    "\n",
    "        predictions = torch.stack(predictions, dim=1)  # (batch_size, n_future, output_size)\n",
    "        attention_weights_list = torch.stack(attention_weights_list, dim=1)  # (batch_size, n_future, seq_len)\n",
    "\n",
    "        return predictions, hidden, cell, attention_weights_list  # Return attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare sequences\n",
    "n_timesteps_input = 12\n",
    "n_future = 5\n",
    "X, y, current_temporal = create_sequences(df[features], df[target], n_timesteps_input, n_future)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test, current_temporal_train, current_temporal_test = train_test_split(\n",
    "    X, y, current_temporal, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "current_temporal_train = torch.tensor(current_temporal_train, dtype=torch.float32)\n",
    "current_temporal_test = torch.tensor(current_temporal_test, dtype=torch.float32)\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = len(features)  # 6 features\n",
    "hidden_size = 128\n",
    "output_size = len(target)  # 2 targets\n",
    "num_layers = 2\n",
    "dropout = 0.3\n",
    "learning_rate = 0.001\n",
    "n_epochs = 50\n",
    "\n",
    "# Initialize model\n",
    "model = LSTMEncoderDecoderWithAttention(input_size, hidden_size, output_size, num_layers, dropout, n_future)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    output, _ = model(X_train, current_temporal_train)\n",
    "    \n",
    "    # Calculate loss\n",
    "    loss = criterion(output, y_train)\n",
    "    \n",
    "    # Backward pass and optimize\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{n_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Example evaluation on test set\n",
    "model.eval()\n",
    "attention_weights_list = []\n",
    "with torch.no_grad():\n",
    "    forecast, attention_weights = model(X_test, current_temporal_test)\n",
    "    attention_weights_list.append(attention_weights.cpu().numpy())\n",
    "\n",
    "# Convert list to numpy array\n",
    "attention_weights_array = np.concatenate(attention_weights_list, axis=0)\n",
    "\n",
    "# Inverse-transform the predictions to the original scale\n",
    "forecast_original_scale = scaler_target.inverse_transform(forecast.cpu().numpy().reshape(-1, forecast.shape[-1])).reshape(forecast.shape)\n",
    "y_test_original_scale = scaler_target.inverse_transform(y_test.cpu().numpy().reshape(-1, y_test.shape[-1])).reshape(y_test.shape)\n",
    "\n",
    "# Compare predictions to the actual values\n",
    "print(\"Predictions on original scale:\", forecast_original_scale)\n",
    "print(\"True values on original scale:\", y_test_original_scale)\n",
    "\n",
    "# Visualize attention weights for a specific sample\n",
    "sample_index = 0  # Change this to visualize different samples\n",
    "attention_weights_sample = attention_weights_array[sample_index]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(attention_weights_sample, cmap='viridis', annot=True)\n",
    "plt.title('Attention Weights for Sample Index {}'.format(sample_index))\n",
    "plt.xlabel('Input Sequence Index')\n",
    "plt.ylabel('Attention Weight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, target_data, n_timesteps, n_future):\n",
    "    X, y, current_time_features = [], [], []\n",
    "    for i in range(len(data) - n_timesteps - n_future + 1):\n",
    "        X.append(data[i:i + n_timesteps].values)\n",
    "        y.append(target_data.iloc[i + n_timesteps:i + n_timesteps + n_future].values)\n",
    "        current_time_features.append(data[current_temporal_features].iloc[i + n_timesteps:i + n_timesteps + n_future].values)\n",
    "    return np.array(X), np.array(y), np.array(current_time_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictable_features = ['rate_level_1', 'rate_level_2', 'days_to_ECB_meeting', 'days_to_Fed_meeting', 'ois_sofr_rate']\n",
    "non_predictable_features = ['days_to_end_of_month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, target_data, n_timesteps, n_future):\n",
    "    X, y, current_time_features, non_predictable_time_features = [], [], [], []\n",
    "    for i in range(len(data) - n_timesteps - n_future + 1):\n",
    "        X.append(data[predictable_features].iloc[i:i + n_timesteps].values)\n",
    "        y.append(target_data.iloc[i + n_timesteps:i + n_timesteps + n_future].values)\n",
    "        current_time_features.append(data[predictable_features].iloc[i + n_timesteps:i + n_timesteps + n_future].values)\n",
    "        non_predictable_time_features.append(data[non_predictable_features].iloc[i + n_timesteps:i + n_timesteps + n_future].values)\n",
    "    return np.array(X), np.array(y), np.array(current_time_features), np.array(non_predictable_time_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMDecoderWithAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, num_layers, dropout, n_future):\n",
    "        super(LSTMDecoderWithAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.n_future = n_future\n",
    "\n",
    "        self.attention = Attention(hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size + len(predictable_features) + len(non_predictable_features), hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, hidden, cell, encoder_outputs, current_temporal_features, non_predictable_features):\n",
    "        # hidden: (num_layers, batch_size, hidden_size)\n",
    "        # encoder_outputs: (batch_size, seq_len, hidden_size)\n",
    "        # current_temporal_features: (batch_size, n_future, len(predictable_features))\n",
    "        # non_predictable_features: (batch_size, n_future, len(non_predictable_features))\n",
    "\n",
    "        predictions = []\n",
    "        attention_weights_list = []\n",
    "\n",
    "        for t in range(self.n_future):\n",
    "            attention_weights = self.attention(hidden[-1], encoder_outputs)  # (batch_size, seq_len)\n",
    "            attention_weights = attention_weights.unsqueeze(1)  # (batch_size, 1, seq_len)\n",
    "            context = torch.bmm(attention_weights, encoder_outputs)  # (batch_size, 1, hidden_size)\n",
    "\n",
    "            # Combine context with current temporal features and non-predictable features\n",
    "            decoder_input = torch.cat([context, current_temporal_features[:, t:t+1, :], non_predictable_features[:, t:t+1, :]], dim=2)  # (batch_size, 1, hidden_size + len(predictable_features) + len(non_predictable_features))\n",
    "            \n",
    "            # Pass through LSTM\n",
    "            output, (hidden, cell) = self.lstm(decoder_input, (hidden, cell))\n",
    "            \n",
    "            # Pass through the final fully connected layer\n",
    "            prediction = self.fc(output).squeeze(1)  # (batch_size, output_size)\n",
    "            predictions.append(prediction)\n",
    "            attention_weights_list.append(attention_weights.squeeze(1))\n",
    "\n",
    "        predictions = torch.stack(predictions, dim=1)  # (batch_size, n_future, output_size)\n",
    "        attention_weights_list = torch.stack(attention_weights_list, dim=1)  # (batch_size, n_future, seq_len)\n",
    "\n",
    "        return predictions, hidden, cell, attention_weights_list  # Return attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare sequences\n",
    "n_timesteps_input = 12\n",
    "n_future = 5\n",
    "X, y, current_temporal, non_predictable_temporal = create_sequences(df, df[target], n_timesteps_input, n_future)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test, current_temporal_train, current_temporal_test, non_predictable_temporal_train, non_predictable_temporal_test = train_test_split(\n",
    "    X, y, current_temporal, non_predictable_temporal, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "current_temporal_train = torch.tensor(current_temporal_train, dtype=torch.float32)\n",
    "current_temporal_test = torch.tensor(current_temporal_test, dtype=torch.float32)\n",
    "non_predictable_temporal_train = torch.tensor(non_predictable_temporal_train, dtype=torch.float32)\n",
    "non_predictable_temporal_test = torch.tensor(non_predictable_temporal_test, dtype=torch.float32)\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = len(predictable_features)  # 5 features\n",
    "hidden_size = 128\n",
    "output_size = len(target)  # 2 targets\n",
    "num_layers = 2\n",
    "dropout = 0.3\n",
    "learning_rate = 0.001\n",
    "n_epochs = 50\n",
    "\n",
    "# Initialize model\n",
    "model = LSTMEncoderDecoderWithAttention(input_size, hidden_size, output_size, num_layers, dropout, n_future)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    output, _ = model(X_train, current_temporal_train, non_predictable_temporal_train)\n",
    "    \n",
    "    # Calculate loss\n",
    "    loss = criterion(output, y_train)\n",
    "    \n",
    "    # Backward pass and optimize\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{n_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Example evaluation on test set\n",
    "model.eval()\n",
    "attention_weights_list = []\n",
    "with torch.no_grad():\n",
    "    forecast, attention_weights = model(X_test, current_temporal_test, non_predictable_temporal_test)\n",
    "    attention_weights_list.append(attention_weights.cpu().numpy())\n",
    "\n",
    "# Convert list to numpy array\n",
    "attention_weights_array = np.concatenate(attention_weights_list, axis=0)\n",
    "\n",
    "# Inverse-transform the predictions to the original scale\n",
    "forecast_original_scale = scaler_target.inverse_transform(forecast.cpu().numpy().reshape(-1, forecast.shape[-1])).reshape(forecast.shape)\n",
    "y_test_original_scale = scaler_target.inverse_transform(y_test.cpu().numpy().reshape(-1, y_test.shape[-1])).reshape(y_test.shape)\n",
    "\n",
    "# Compare predictions to the actual values\n",
    "print(\"Predictions on original scale:\", forecast_original_scale)\n",
    "print(\"True values on original scale:\", y_test_original_scale)\n",
    "\n",
    "# Visualize attention weights for a specific sample\n",
    "sample_index = 0  # Change this to visualize different samples\n",
    "attention_weights_sample = attention_weights_array[sample_index]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(attention_weights_sample, cmap='viridis', annot=True)\n",
    "plt.title('Attention Weights for Sample Index {}'.format(sample_index))\n",
    "plt.xlabel('Input Sequence Index')\n",
    "plt.ylabel('Attention Weight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sample list of dates\n",
    "dates = ['2023-01-15', '2023-02-20', '2023-03-25', '2023-04-30']\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame({'date': pd.to_datetime(dates)})\n",
    "\n",
    "# Extract month as 'MMM'\n",
    "df['month'] = df['date'].dt.strftime('%b')\n",
    "\n",
    "# One-Hot Encoding\n",
    "one_hot_encoded = pd.get_dummies(df['month'])\n",
    "\n",
    "# Ordinal Encoding\n",
    "ordinal_mapping = {month: i for i, month in enumerate(['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'], 1)}\n",
    "df['month_ordinal'] = df['month'].map(ordinal_mapping)\n",
    "\n",
    "# Sine and Cosine Encoding\n",
    "df['month_num'] = df['date'].dt.month\n",
    "df['month_sin'] = np.sin(2 * np.pi * df['month_num'] / 12)\n",
    "df['month_cos'] = np.cos(2 * np.pi * df['month_num'] / 12)\n",
    "\n",
    "print(\"One-Hot Encoded:\\n\", one_hot_encoded)\n",
    "print(\"Ordinal Encoded:\\n\", df[['month', 'month_ordinal']])\n",
    "print(\"Sine and Cosine Encoded:\\n\", df[['month', 'month_sin', 'month_cos']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "One-Hot Encoding:\n",
    "\n",
    "This method creates a binary vector for each month. For example, January would be [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], February would be [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], and so on.\n",
    "Ordinal Encoding:\n",
    "\n",
    "This method assigns an integer to each month. For example, January would be 1, February would be 2, and so on.\n",
    "Sine and Cosine Encoding:\n",
    "\n",
    "This method encodes the month as two continuous features using sine and cosine functions. This is useful for capturing the cyclical nature of months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "dates = ['2021-01-01', '2021-02-01', '2021-03-01', '2021-04-01', '2021-05-01', '2021-06-01',\n",
    "         '2021-07-01', '2021-08-01', '2021-09-01', '2021-10-01', '2021-11-01', '2021-12-01']\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame({'date': pd.to_datetime(dates)})\n",
    "\n",
    "# Extract month number\n",
    "df['month_num'] = df['date'].dt.month\n",
    "\n",
    "# Sine and Cosine Encoding\n",
    "df['month_sin'] = np.sin(2 * np.pi * df['month_num'] / 12)\n",
    "df['month_cos'] = np.cos(2 * np.pi * df['month_num'] / 12)\n",
    "\n",
    "print(\"Sine and Cosine Encoded:\\n\", df[['date', 'month_sin', 'month_cos']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      cut_id  stub_rate  first_meeting_days  first_meeting_rate  \\\n",
      "0 2021-10-01     0.0001                  33              0.0002   \n",
      "1 2021-10-04     0.0005                  30              0.0002   \n",
      "2 2021-10-05     0.0005                  29              0.0002   \n",
      "\n",
      "   second_meeting_days  second_meeting_rate  \n",
      "0                   61               0.0003  \n",
      "1                   58               0.0003  \n",
      "2                   57               0.0003  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    'cut_id': ['2021-10-01', '2021-10-01', '2021-10-01', '2021-10-01', '2021-10-01', '2021-10-01', '2021-10-01', '2021-10-01', '2021-10-01',\n",
    "               '2021-10-04', '2021-10-04', '2021-10-04', '2021-10-04', '2021-10-04', '2021-10-04', '2021-10-04', '2021-10-04', '2021-10-04',\n",
    "               '2021-10-05', '2021-10-05', '2021-10-05', '2021-10-05', '2021-10-05'],\n",
    "    'name': [\"b'USD Stub'\", \"b'USD 2021-11-03'\", \"b'USD 2021-12-01'\", \"b'USD 2022-01-03'\", \"b'USD 2022-02-01'\", \"b'USD 2022-03-01'\", \"b'USD 2022-04-01'\", \"b'USD 2022-05-02'\", \"b'USD 2022-06-01'\",\n",
    "             \"b'USD Stub'\", \"b'USD 2021-11-03'\", \"b'USD 2021-12-01'\", \"b'USD 2022-01-03'\", \"b'USD 2022-02-01'\", \"b'USD 2022-03-01'\", \"b'USD 2022-04-01'\", \"b'USD 2022-05-02'\", \"b'USD 2022-06-01'\",\n",
    "             \"b'USD Stub'\", \"b'USD 2021-11-03'\", \"b'USD 2021-12-01'\", \"b'USD 2022-01-03'\", \"b'USD 2022-02-01'\"],\n",
    "    'rate': [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008, 0.0009,\n",
    "             0.0005, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008, 0.0009,\n",
    "             0.0005, 0.0002, 0.0003, 0.0004, 0.0005]\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Function to extract rates and meeting dates\n",
    "def extract_meeting_info(df):\n",
    "    # Ensure the date columns are in datetime format\n",
    "    df['cut_id'] = pd.to_datetime(df['cut_id'])\n",
    "\n",
    "    # Function to parse meeting dates\n",
    "    def parse_meeting_date(name):\n",
    "        try:\n",
    "            # Extract the date part from the string\n",
    "            date_str = name.split()[1][0:-1]\n",
    "            return pd.to_datetime(date_str)\n",
    "        except Exception as e:\n",
    "            return None\n",
    "\n",
    "    # Extract meeting dates\n",
    "    df['meeting_date'] = df['name'].apply(lambda x: parse_meeting_date(x) if 'USD' in x else None)\n",
    "\n",
    "    # Ensure meeting_date column is in datetime format\n",
    "    df['meeting_date'] = pd.to_datetime(df['meeting_date'], errors='coerce')\n",
    "\n",
    "    # Initialize lists to store the results\n",
    "    stub_rate_list = []\n",
    "    first_meeting_days_list = []\n",
    "    first_meeting_rate_list = []\n",
    "    second_meeting_days_list = []\n",
    "    second_meeting_rate_list = []\n",
    "\n",
    "    # Get unique cut_ids\n",
    "    unique_cut_ids = df['cut_id'].unique()\n",
    "\n",
    "    # Iterate over each unique cut_id\n",
    "    for cut_id in unique_cut_ids:\n",
    "        # Filter the DataFrame for the current cut_id\n",
    "        current_df = df[df['cut_id'] == cut_id]\n",
    "\n",
    "        # Extract the stub rate\n",
    "        stub_rate = current_df[current_df['name'] == \"b'USD Stub'\"]['rate'].values[0]\n",
    "\n",
    "        # Extract the first and second meeting dates and rates\n",
    "        meeting_dates = current_df[current_df['name'] != \"b'USD Stub'\"].sort_values(by='meeting_date')\n",
    "        if len(meeting_dates) >= 2:\n",
    "            first_meeting_days = (meeting_dates.iloc[0]['meeting_date'] - cut_id).days\n",
    "            first_meeting_rate = meeting_dates.iloc[0]['rate']\n",
    "            second_meeting_days = (meeting_dates.iloc[1]['meeting_date'] - cut_id).days\n",
    "            second_meeting_rate = meeting_dates.iloc[1]['rate']\n",
    "        elif len(meeting_dates) == 1:\n",
    "            first_meeting_days = (meeting_dates.iloc[0]['meeting_date'] - cut_id).days\n",
    "            first_meeting_rate = meeting_dates.iloc[0]['rate']\n",
    "            second_meeting_days = None\n",
    "            second_meeting_rate = None\n",
    "        else:\n",
    "            first_meeting_days = None\n",
    "            first_meeting_rate = None\n",
    "            second_meeting_days = None\n",
    "            second_meeting_rate = None\n",
    "\n",
    "        # Append the results to the lists\n",
    "        stub_rate_list.append(stub_rate)\n",
    "        first_meeting_days_list.append(first_meeting_days)\n",
    "        first_meeting_rate_list.append(first_meeting_rate)\n",
    "        second_meeting_days_list.append(second_meeting_days)\n",
    "        second_meeting_rate_list.append(second_meeting_rate)\n",
    "\n",
    "    # Create a new DataFrame with the results\n",
    "    result_df = pd.DataFrame({\n",
    "        'cut_id': unique_cut_ids,\n",
    "        'stub_rate': stub_rate_list,\n",
    "        'first_meeting_days': first_meeting_days_list,\n",
    "        'first_meeting_rate': first_meeting_rate_list,\n",
    "        'second_meeting_days': second_meeting_days_list,\n",
    "        'second_meeting_rate': second_meeting_rate_list\n",
    "    })\n",
    "\n",
    "    return result_df\n",
    "\n",
    "# Extract meeting info\n",
    "result_df = extract_meeting_info(df)\n",
    "print(result_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
