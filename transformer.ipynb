{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMTransformerEncoderDecoder(nn.Module):\n",
    "    def __init__(self, input_dim, macro_dim, embed_dim, lstm_hidden_dim, lstm_layers, num_heads, num_layers, dropout=0.1, output_seq_len=1):\n",
    "        super(LSTMTransformerEncoderDecoder, self).__init__()\n",
    "        \n",
    "        # LSTM Encoder for past sequence data\n",
    "        self.lstm_encoder = nn.LSTM(input_dim, lstm_hidden_dim, lstm_layers, batch_first=True)\n",
    "        \n",
    "        # Linear layer to embed LSTM output to Transformer-compatible dimension\n",
    "        self.past_embedding = nn.Linear(lstm_hidden_dim, embed_dim)\n",
    "        \n",
    "        # Macro data embedding\n",
    "        self.macro_embedding = nn.Linear(macro_dim, embed_dim)\n",
    "        \n",
    "        # Positional encoding for the past sequence\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, 1000, embed_dim))  # Assuming max sequence length of 1000\n",
    "        \n",
    "        # Transformer Encoder layers\n",
    "        self.transformer_layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dropout=dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # LSTM Decoder\n",
    "        self.lstm_decoder = nn.LSTM(embed_dim, lstm_hidden_dim, lstm_layers, batch_first=True)\n",
    "        \n",
    "        # Linear output layer\n",
    "        self.output_layer = nn.Linear(lstm_hidden_dim, input_dim)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Output sequence length for forecasting multiple steps\n",
    "        self.output_seq_len = output_seq_len\n",
    "        \n",
    "    def forward(self, past_sequence, macro_today):\n",
    "        \"\"\"\n",
    "        past_sequence: (batch_size, sequence_length, input_dim) - historical sequence data\n",
    "        macro_today: (batch_size, macro_dim) - today's macro data\n",
    "        \"\"\"\n",
    "        \n",
    "        # LSTM Encoder: process the past sequence data\n",
    "        lstm_out, (h_n, c_n) = self.lstm_encoder(past_sequence)  # lstm_out: (batch_size, sequence_length, lstm_hidden_dim)\n",
    "        \n",
    "        # Embed the LSTM output to transformer-compatible dimension\n",
    "        past_embedded = self.past_embedding(lstm_out)  # (batch_size, sequence_length, embed_dim)\n",
    "        \n",
    "        # Add positional encodings to past sequence\n",
    "        seq_length = past_embedded.size(1)\n",
    "        past_embedded = past_embedded + self.positional_encoding[:, :seq_length, :]\n",
    "        \n",
    "        # Embed todayâ€™s macro data and expand for broadcasting\n",
    "        macro_today_embedded = self.macro_embedding(macro_today)  # (batch_size, embed_dim)\n",
    "        macro_today_embedded = macro_today_embedded.unsqueeze(1)  # (batch_size, 1, embed_dim)\n",
    "        \n",
    "        # Concatenate past and macro embeddings to allow today's data to influence predictions\n",
    "        x = torch.cat([past_embedded, macro_today_embedded], dim=1)  # (batch_size, sequence_length + 1, embed_dim)\n",
    "        \n",
    "        # Transpose for transformer input (sequence_length + 1, batch_size, embed_dim)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        \n",
    "        # Pass through transformer layers\n",
    "        for layer in self.transformer_layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        # Transpose back to (batch_size, sequence_length + 1, embed_dim)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        \n",
    "        # LSTM Decoder for autoregressive prediction\n",
    "        decoder_input = x[:, -1, :].unsqueeze(1)  # Start with the output of the last transformer step\n",
    "        \n",
    "        # Collect decoder outputs\n",
    "        decoder_outputs = []\n",
    "        hidden_state, cell_state = h_n, c_n  # Initialize with the encoder LSTM's final hidden states\n",
    "        \n",
    "        for _ in range(self.output_seq_len):\n",
    "            # Pass through LSTM Decoder one step at a time\n",
    "            decoder_output, (hidden_state, cell_state) = self.lstm_decoder(decoder_input, (hidden_state, cell_state))\n",
    "            \n",
    "            # Apply output layer to get the forecast for this step\n",
    "            step_output = self.output_layer(decoder_output.squeeze(1))  # (batch_size, input_dim)\n",
    "            decoder_outputs.append(step_output)\n",
    "            \n",
    "            # Prepare next input (autoregressive)\n",
    "            decoder_input = decoder_output  # Feed the last output as the next input\n",
    "            \n",
    "        # Stack all the step outputs\n",
    "        final_output = torch.stack(decoder_outputs, dim=1)  # (batch_size, output_seq_len, input_dim)\n",
    "        \n",
    "        return final_output\n",
    "\n",
    "# Model configuration\n",
    "input_dim = 5                # Number of input features\n",
    "macro_dim = 3                # Number of macroeconomic variables\n",
    "embed_dim = 64               # Embedding dimension for Transformer\n",
    "lstm_hidden_dim = 128        # Hidden dimension for LSTM\n",
    "lstm_layers = 2              # Number of LSTM layers\n",
    "num_heads = 4                # Number of attention heads in Transformer\n",
    "num_layers = 2               # Number of Transformer layers\n",
    "dropout = 0.1                # Dropout rate\n",
    "output_seq_len = 5           # Number of forecasted steps\n",
    "\n",
    "# Instantiate the model\n",
    "model = LSTMTransformerEncoderDecoder(input_dim, macro_dim, embed_dim, lstm_hidden_dim, lstm_layers, num_heads, num_layers, dropout, output_seq_len)\n",
    "\n",
    "# Example input (batch_size=32, sequence_length=10, input_dim=5)\n",
    "past_sequence = torch.randn(32, 10, input_dim)  # Historical sequence data\n",
    "macro_today = torch.randn(32, macro_dim)        # Today's macroeconomic data\n",
    "\n",
    "# Get the output prediction\n",
    "output = model(past_sequence, macro_today)\n",
    "print(output.shape)  # Expected: (32, output_seq_len, input_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "\n",
    "def train_model(model, dataset, optimizer, criterion, config):\n",
    "    # Initialize distributed process group if DDP is enabled\n",
    "    ddp = int(os.environ.get('RANK', -1)) != -1\n",
    "    if ddp:\n",
    "        init_process_group(backend=config['backend'])\n",
    "        ddp_rank = int(os.environ['RANK'])\n",
    "        ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
    "        ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
    "        device = f'cuda:{ddp_local_rank}'\n",
    "        torch.cuda.set_device(device)\n",
    "        model = DDP(model, device_ids=[ddp_local_rank])\n",
    "    else:\n",
    "        device = config['device']\n",
    "\n",
    "    # Set model, optimizer, and criterion to specified device\n",
    "    model.to(device)\n",
    "    criterion.to(device)\n",
    "\n",
    "    # Prepare DataLoader for batching\n",
    "    batch_size = config['batch_size']\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "\n",
    "    # Training loop\n",
    "    total_loss = 0.0\n",
    "    model.train()\n",
    "    for epoch in range(config['num_epochs']):\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, targets) in enumerate(dataloader):\n",
    "            # Move data to the appropriate device\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping (optional)\n",
    "            if config.get('grad_clip', 0.0) > 0.0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), config['grad_clip'])\n",
    "\n",
    "            # Step the optimizer\n",
    "            optimizer.step()\n",
    "\n",
    "            # Track running loss for logging\n",
    "            running_loss += loss.item()\n",
    "            if (i + 1) % config['log_interval'] == 0:\n",
    "                print(f\"Epoch [{epoch + 1}/{config['num_epochs']}], Step [{i + 1}/{len(dataloader)}], \"\n",
    "                      f\"Loss: {running_loss / config['log_interval']:.4f}\")\n",
    "                running_loss = 0.0\n",
    "\n",
    "            # Optionally, evaluate and save checkpoints\n",
    "            if config['eval_interval'] > 0 and i % config['eval_interval'] == 0:\n",
    "                evaluate_and_checkpoint(model, optimizer, device, config)\n",
    "\n",
    "        # Update total loss for epoch\n",
    "        total_loss += running_loss\n",
    "\n",
    "    # Clean up DDP process group if necessary\n",
    "    if ddp:\n",
    "        destroy_process_group()\n",
    "\n",
    "    return total_loss / config['num_epochs']\n",
    "\n",
    "\n",
    "def evaluate_and_checkpoint(model, optimizer, device, config):\n",
    "    model.eval()\n",
    "    eval_loss = 0.0\n",
    "    # Dummy evaluation loop (replace with actual validation data and logic)\n",
    "    with torch.no_grad():\n",
    "        for _ in range(config['eval_iters']):\n",
    "            # Perform evaluation here; set eval_loss appropriately\n",
    "            pass\n",
    "\n",
    "    print(f\"Eval loss: {eval_loss}\")\n",
    "    model.train()\n",
    "\n",
    "    # Save model checkpoint\n",
    "    checkpoint_path = os.path.join(config['out_dir'], f\"checkpoint_epoch_{config['current_epoch']}.pt\")\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, checkpoint_path)\n",
    "    print(f\"Checkpoint saved at {checkpoint_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "class LSTMTransformerEncoderDecoder(nn.Module):\n",
    "    def __init__(self, input_dim, macro_dim, embed_dim, lstm_hidden_dim, lstm_layers, num_heads, num_layers, dropout=0.1, output_seq_len=1):\n",
    "        super(LSTMTransformerEncoderDecoder, self).__init__()\n",
    "        \n",
    "        # Model architecture as previously defined\n",
    "        # ...\n",
    "\n",
    "    def forward(self, past_sequence, macro_today):\n",
    "        # Forward pass logic as defined in the previous model\n",
    "        # ...\n",
    "\n",
    "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
    "        # Collect model parameters\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        \n",
    "        # Filter out parameters that do not require gradient computation\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "        \n",
    "        # Separate parameters into decay and no-decay groups\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        \n",
    "        # Define optimizer parameter groups\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        \n",
    "        # Print parameter count information\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
    "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
    "        \n",
    "        # Check if the fused AdamW optimizer is available for CUDA devices\n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and device_type == 'cuda'\n",
    "        extra_args = dict(fused=True) if use_fused else dict()\n",
    "        \n",
    "        # Initialize the AdamW optimizer\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
    "        \n",
    "        print(f\"Using fused AdamW: {use_fused}\")\n",
    "        \n",
    "        return optimizer\n",
    "\n",
    "# Example usage\n",
    "device_type = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = LSTMTransformerEncoderDecoder(input_dim, macro_dim, embed_dim, lstm_hidden_dim, lstm_layers, num_heads, num_layers, dropout, output_seq_len)\n",
    "optimizer = model.configure_optimizers(weight_decay=0.01, learning_rate=1e-4, betas=(0.9, 0.999), device_type=device_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "\n",
    "def train_model(model, dataset, optimizer, criterion, config):\n",
    "    # Distributed Data Parallel (DDP) initialization\n",
    "    ddp = int(os.environ.get('RANK', -1)) != -1\n",
    "    if ddp:\n",
    "        init_process_group(backend=config['backend'])\n",
    "        ddp_rank = int(os.environ['RANK'])\n",
    "        ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
    "        device = f'cuda:{ddp_local_rank}'\n",
    "        torch.cuda.set_device(device)\n",
    "        model = DDP(model, device_ids=[ddp_local_rank])\n",
    "    else:\n",
    "        device = config['device']\n",
    "\n",
    "    model.to(device)\n",
    "    criterion.to(device)\n",
    "\n",
    "    # Prepare DataLoader for time series batching\n",
    "    batch_size = config['batch_size']\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "\n",
    "    total_loss = 0.0\n",
    "    model.train()\n",
    "    for epoch in range(config['num_epochs']):\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, targets) in enumerate(dataloader):\n",
    "            # Move inputs and targets to the correct device\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            # Forward pass for hybrid model\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)  # Ensure model handles the time series input shape properly\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "\n",
    "            # Optional: gradient clipping for stability\n",
    "            if config.get('grad_clip', 0.0) > 0.0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), config['grad_clip'])\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if (i + 1) % config['log_interval'] == 0:\n",
    "                print(f\"Epoch [{epoch + 1}/{config['num_epochs']}], Step [{i + 1}/{len(dataloader)}], \"\n",
    "                      f\"Loss: {running_loss / config['log_interval']:.4f}\")\n",
    "                running_loss = 0.0\n",
    "\n",
    "            # Optional: evaluation and checkpointing\n",
    "            if config['eval_interval'] > 0 and i % config['eval_interval'] == 0:\n",
    "                evaluate_and_checkpoint(model, optimizer, device, config)\n",
    "\n",
    "        total_loss += running_loss\n",
    "\n",
    "    if ddp:\n",
    "        destroy_process_group()\n",
    "\n",
    "    return total_loss / config['num_epochs']\n",
    "\n",
    "\n",
    "def evaluate_and_checkpoint(model, optimizer, device, config):\n",
    "    model.eval()\n",
    "    eval_loss = 0.0\n",
    "    # Dummy evaluation loop; replace with actual validation set and metrics\n",
    "    with torch.no_grad():\n",
    "        for _ in range(config['eval_iters']):\n",
    "            # Perform evaluation on validation data here\n",
    "            pass\n",
    "\n",
    "    print(f\"Eval loss: {eval_loss}\")\n",
    "    model.train()\n",
    "\n",
    "    # Save model checkpoint\n",
    "    checkpoint_path = os.path.join(config['out_dir'], f\"checkpoint_epoch_{config['current_epoch']}.pt\")\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, checkpoint_path)\n",
    "    print(f\"Checkpoint saved at {checkpoint_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMTransformerEncoderDecoder(nn.Module):\n",
    "    def __init__(self, input_dim, macro_dim, embed_dim, lstm_hidden_dim, lstm_layers, num_heads, num_layers, dropout=0.1, output_seq_len=1):\n",
    "        super(LSTMTransformerEncoderDecoder, self).__init__()\n",
    "        \n",
    "        # LSTM Encoder for past sequence data\n",
    "        self.lstm_encoder = nn.LSTM(input_dim, lstm_hidden_dim, lstm_layers, batch_first=True)\n",
    "        \n",
    "        # Linear layer to embed LSTM output to Transformer-compatible dimension\n",
    "        self.past_embedding = nn.Linear(lstm_hidden_dim, embed_dim)\n",
    "        \n",
    "        # Macro data embedding\n",
    "        self.macro_embedding = nn.Linear(macro_dim, embed_dim)\n",
    "        \n",
    "        # Transformer Encoder layers\n",
    "        self.transformer_layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dropout=dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # LSTM Decoder\n",
    "        self.lstm_decoder = nn.LSTM(embed_dim, lstm_hidden_dim, lstm_layers, batch_first=True)\n",
    "        \n",
    "        # Linear output layer\n",
    "        self.output_layer = nn.Linear(lstm_hidden_dim, input_dim)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Output sequence length for forecasting multiple steps\n",
    "        self.output_seq_len = output_seq_len\n",
    "        \n",
    "    def get_positional_encoding(self, seq_len, embed_dim, device):\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1).to(device)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2, dtype=torch.float) * -(torch.log(torch.tensor(10000.0)) / embed_dim)).to(device)\n",
    "        \n",
    "        pe = torch.zeros(seq_len, embed_dim, device=device)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        return pe.unsqueeze(0)  # Shape: (1, seq_len, embed_dim)\n",
    "\n",
    "    def forward(self, past_sequence, macro_today):\n",
    "        \"\"\"\n",
    "        past_sequence: (batch_size, sequence_length, input_dim) - historical sequence data\n",
    "        macro_today: (batch_size, macro_dim) - today's macro data\n",
    "        \"\"\"\n",
    "        device = past_sequence.device\n",
    "        \n",
    "        # LSTM Encoder: process the past sequence data\n",
    "        lstm_out, (h_n, c_n) = self.lstm_encoder(past_sequence)  # lstm_out: (batch_size, sequence_length, lstm_hidden_dim)\n",
    "        \n",
    "        # Embed the LSTM output to transformer-compatible dimension\n",
    "        past_embedded = self.past_embedding(lstm_out)  # (batch_size, sequence_length, embed_dim)\n",
    "        \n",
    "        # Generate dynamic positional encoding based on actual sequence length\n",
    "        seq_length = past_embedded.size(1)\n",
    "        positional_encoding = self.get_positional_encoding(seq_length, past_embedded.size(2), device)\n",
    "        \n",
    "        # Add positional encodings to past sequence\n",
    "        past_embedded = past_embedded + positional_encoding\n",
    "        \n",
    "        # Embed todayâ€™s macro data and expand for broadcasting\n",
    "        macro_today_embedded = self.macro_embedding(macro_today)  # (batch_size, embed_dim)\n",
    "        macro_today_embedded = macro_today_embedded.unsqueeze(1)  # (batch_size, 1, embed_dim)\n",
    "        \n",
    "        # Concatenate past and macro embeddings to allow today's data to influence predictions\n",
    "        x = torch.cat([past_embedded, macro_today_embedded], dim=1)  # (batch_size, sequence_length + 1, embed_dim)\n",
    "        \n",
    "        # Transpose for transformer input (sequence_length + 1, batch_size, embed_dim)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        \n",
    "        # Pass through transformer layers\n",
    "        for layer in self.transformer_layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        # Transpose back to (batch_size, sequence_length + 1, embed_dim)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        \n",
    "        # LSTM Decoder for autoregressive prediction\n",
    "        decoder_input = x[:, -1, :].unsqueeze(1)  # Start with the output of the last transformer step\n",
    "        \n",
    "        # Collect decoder outputs\n",
    "        decoder_outputs = []\n",
    "        hidden_state, cell_state = h_n, c_n  # Initialize with the encoder LSTM's final hidden states\n",
    "        \n",
    "        for _ in range(self.output_seq_len):\n",
    "            # Pass through LSTM Decoder one step at a time\n",
    "            decoder_output, (hidden_state, cell_state) = self.lstm_decoder(decoder_input, (hidden_state, cell_state))\n",
    "            \n",
    "            # Apply output layer to get the forecast for this step\n",
    "            step_output = self.output_layer(decoder_output.squeeze(1))  # (batch_size, input_dim)\n",
    "            decoder_outputs.append(step_output)\n",
    "            \n",
    "            # Prepare next input (autoregressive)\n",
    "            decoder_input = decoder_output  # Feed the last output as the next input\n",
    "            \n",
    "        # Stack all the step outputs\n",
    "        final_output = torch.stack(decoder_outputs, dim=1)  # (batch_size, output_seq_len, input_dim)\n",
    "        \n",
    "        return final_output\n",
    "\n",
    "# Model configuration\n",
    "input_dim = 5                # Number of input features\n",
    "macro_dim = 3                # Number of macroeconomic variables\n",
    "embed_dim = 64               # Embedding dimension for Transformer\n",
    "lstm_hidden_dim = 128        # Hidden dimension for LSTM\n",
    "lstm_layers = 2              # Number of LSTM layers\n",
    "num_heads = 4                # Number of attention heads in Transformer\n",
    "num_layers = 2               # Number of Transformer layers\n",
    "dropout = 0.1                # Dropout rate\n",
    "output_seq_len = 5           # Number of forecasted steps\n",
    "\n",
    "# Instantiate the model\n",
    "model = LSTMTransformerEncoderDecoder(input_dim, macro_dim, embed_dim, lstm_hidden_dim, lstm_layers, num_heads, num_layers, dropout, output_seq_len)\n",
    "\n",
    "# Example input (batch_size=32, sequence_length=10, input_dim=5)\n",
    "past_sequence = torch.randn(32, 10, input_dim)  # Historical sequence data\n",
    "macro_today = torch.randn(32, macro_dim)        # Today's macroeconomic data\n",
    "\n",
    "# Get the output prediction\n",
    "output = model(past_sequence, macro_today)\n",
    "print(output.shape)  # Expected: (32, output_seq_len, input_dim)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
