{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Define Attention Mechanism\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.attn.weight)\n",
    "        nn.init.constant_(self.attn.bias, 0)\n",
    "        nn.init.uniform_(self.v, -0.1, 0.1)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        seq_len = encoder_outputs.size(1)\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, seq_len, 1)\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "        energy = energy.transpose(1, 2)\n",
    "        v = self.v.repeat(encoder_outputs.size(0), 1).unsqueeze(1)\n",
    "        attention_weights = torch.bmm(v, energy).squeeze(1)\n",
    "        return torch.softmax(attention_weights, dim=1)\n",
    "\n",
    "# Define LSTM Encoder\n",
    "class LSTMEncoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout):\n",
    "        super(LSTMEncoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs, (hidden, cell) = self.lstm(x)\n",
    "        return outputs, hidden, cell\n",
    "\n",
    "# Define LSTM Decoder with Attention\n",
    "class LSTMDecoderWithAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, num_layers, dropout):\n",
    "        super(LSTMDecoderWithAttention, self).__init__()\n",
    "        self.attention = Attention(hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size * 2, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, encoder_outputs, hidden, cell, targets=None, teacher_forcing_ratio=0.5):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        seq_len = encoder_outputs.size(1)\n",
    "        output_size = self.fc.out_features\n",
    "\n",
    "        outputs = torch.zeros(batch_size, seq_len, output_size).to(encoder_outputs.device)\n",
    "        input = encoder_outputs[:, -1, :].unsqueeze(1)  # First input to the decoder is the last hidden state of the encoder\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            attn_weights = self.attention(hidden[-1], encoder_outputs)\n",
    "            context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n",
    "            lstm_input = torch.cat([context, input.squeeze(1)], dim=1).unsqueeze(1)  # Add sequence dimension\n",
    "            output, (hidden, cell) = self.lstm(lstm_input, (hidden, cell))\n",
    "            output = self.fc(output.squeeze(1))\n",
    "            outputs[:, t, :] = output\n",
    "\n",
    "            teacher_force = targets is not None and torch.rand(1).item() < teacher_forcing_ratio\n",
    "            input = targets[:, t].unsqueeze(1) if teacher_force else output.unsqueeze(1)\n",
    "\n",
    "        return outputs, attn_weights\n",
    "\n",
    "# Define the Hybrid Model\n",
    "class HybridModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, dropout):\n",
    "        super(HybridModel, self).__init__()\n",
    "        self.encoder = LSTMEncoder(input_size, hidden_size, num_layers, dropout)\n",
    "        self.decoder = LSTMDecoderWithAttention(hidden_size, output_size, num_layers, dropout)\n",
    "\n",
    "    def forward(self, past_data, targets=None, teacher_forcing_ratio=0.5):\n",
    "        encoder_outputs, hidden, cell = self.encoder(past_data)\n",
    "        prediction, attn_weights = self.decoder(encoder_outputs, hidden, cell, targets, teacher_forcing_ratio)\n",
    "        return prediction, attn_weights\n",
    "\n",
    "def moving_average(data, window_size):\n",
    "    return np.convolve(data, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "# Example usage\n",
    "input_size = 10\n",
    "hidden_size = 128\n",
    "output_size = 1\n",
    "num_layers = 2\n",
    "dropout = 0.5\n",
    "batch_size = 32\n",
    "seq_len = 20\n",
    "window_size = 3  # Window size for moving average\n",
    "\n",
    "model = HybridModel(input_size, hidden_size, output_size, num_layers, dropout)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Dummy data\n",
    "past_data = torch.randn(batch_size, seq_len, input_size)\n",
    "targets = torch.randn(batch_size, seq_len, output_size)\n",
    "\n",
    "# Denoise data using moving average\n",
    "past_data_np = past_data.numpy()\n",
    "denoised_data = np.apply_along_axis(moving_average, 1, past_data_np, window_size)\n",
    "# Pad the denoised data to match the original length\n",
    "denoised_data = np.pad(denoised_data, ((0, 0), (window_size-1, 0), (0, 0)), mode='edge')\n",
    "past_data = torch.tensor(denoised_data, dtype=torch.float32)\n",
    "\n",
    "# Forward pass\n",
    "outputs, attn_weights = model(past_data, targets, teacher_forcing_ratio=0.5)\n",
    "loss = criterion(outputs, targets)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print(\"Output shape:\", outputs.shape)\n",
    "print(\"Attention weights shape:\", attn_weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improving the performance of an LSTM model to match or exceed that of a momentum strategy involves several steps, including hyperparameter tuning, feature engineering, and model optimization. Here are some strategies to enhance the performance of your LSTM model:\n",
    "\n",
    "1. Hyperparameter Tuning\n",
    "Learning Rate: Experiment with different learning rates to find the optimal value.\n",
    "Batch Size: Adjust the batch size to improve training stability and performance.\n",
    "Number of Layers: Experiment with different numbers of LSTM layers.\n",
    "Hidden Units: Adjust the number of hidden units in each LSTM layer.\n",
    "Dropout: Add dropout layers to prevent overfitting.\n",
    "2. Feature Engineering\n",
    "Lag Features: Create lag features to capture temporal dependencies.\n",
    "Rolling Statistics: Add rolling mean, rolling standard deviation, and other statistical features.\n",
    "Seasonal Features: Include seasonal indicators such as day of the week, month, etc.\n",
    "3. Model Optimization\n",
    "Regularization: Use L2 regularization to prevent overfitting.\n",
    "Early Stopping: Implement early stopping to prevent overfitting and reduce training time.\n",
    "Ensemble Methods: Combine multiple LSTM models or use ensemble methods like bagging and boosting.\n",
    "4. Data Preprocessing\n",
    "Normalization: Normalize the input data to improve training stability.\n",
    "Denoising: Apply denoising techniques to remove noise from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import trange\n",
    "\n",
    "# Define Attention Mechanism\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.attn.weight)\n",
    "        nn.init.constant_(self.attn.bias, 0)\n",
    "        nn.init.uniform_(self.v, -0.1, 0.1)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        seq_len = encoder_outputs.size(1)\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, seq_len, 1)\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "        energy = energy.transpose(1, 2)\n",
    "        v = self.v.repeat(encoder_outputs.size(0), 1).unsqueeze(1)\n",
    "        attention_weights = torch.bmm(v, energy).squeeze(1)\n",
    "        return torch.softmax(attention_weights, dim=1)\n",
    "\n",
    "# Define LSTM Encoder\n",
    "class LSTMEncoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout):\n",
    "        super(LSTMEncoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs, (hidden, cell) = self.lstm(x)\n",
    "        outputs = self.dropout(outputs)\n",
    "        return outputs, hidden, cell\n",
    "\n",
    "# Define LSTM Decoder with Attention\n",
    "class LSTMDecoderWithAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, num_layers, dropout):\n",
    "        super(LSTMDecoderWithAttention, self).__init__()\n",
    "        self.attention = Attention(hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size * 2, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, encoder_outputs, hidden, cell, targets=None, teacher_forcing_ratio=0.5):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        seq_len = encoder_outputs.size(1)\n",
    "        output_size = self.fc.out_features\n",
    "\n",
    "        outputs = torch.zeros(batch_size, seq_len, output_size).to(encoder_outputs.device)\n",
    "        input = encoder_outputs[:, -1, :].unsqueeze(1)  # First input to the decoder is the last hidden state of the encoder\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            attn_weights = self.attention(hidden[-1], encoder_outputs)\n",
    "            context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n",
    "            lstm_input = torch.cat([context, input.squeeze(1)], dim=1).unsqueeze(1)  # Add sequence dimension\n",
    "            output, (hidden, cell) = self.lstm(lstm_input, (hidden, cell))\n",
    "            output = self.fc(self.dropout(output.squeeze(1)))\n",
    "            outputs[:, t, :] = output\n",
    "\n",
    "            teacher_force = targets is not None and torch.rand(1).item() < teacher_forcing_ratio\n",
    "            input = targets[:, t].unsqueeze(1) if teacher_force else output.unsqueeze(1)\n",
    "\n",
    "        return outputs, attn_weights\n",
    "\n",
    "# Define the Hybrid Model\n",
    "class HybridModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, dropout):\n",
    "        super(HybridModel, self).__init__()\n",
    "        self.encoder = LSTMEncoder(input_size, hidden_size, num_layers, dropout)\n",
    "        self.decoder = LSTMDecoderWithAttention(hidden_size, output_size, num_layers, dropout)\n",
    "\n",
    "    def forward(self, past_data, targets=None, teacher_forcing_ratio=0.5):\n",
    "        encoder_outputs, hidden, cell = self.encoder(past_data)\n",
    "        prediction, attn_weights = self.decoder(encoder_outputs, hidden, cell, targets, teacher_forcing_ratio)\n",
    "        return prediction, attn_weights\n",
    "\n",
    "def moving_average(data, window_size):\n",
    "    return np.convolve(data, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "def add_lag_features(data, max_lag):\n",
    "    lagged_data = np.zeros((data.shape[0], data.shape[1], data.shape[2] * (max_lag + 1)))\n",
    "    for lag in range(max_lag + 1):\n",
    "        lagged_data[:, lag:, lag * data.shape[2]:(lag + 1) * data.shape[2]] = data[:, :data.shape[1] - lag, :]\n",
    "    return lagged_data\n",
    "\n",
    "# Read data from CSV\n",
    "data = pd.read_csv('time_series_data.csv', index_col='date', parse_dates=True)\n",
    "features = data.columns.tolist()\n",
    "input_size = len(features)\n",
    "\n",
    "# Convert to numpy array\n",
    "data_np = data.values\n",
    "\n",
    "# Denoise data using moving average\n",
    "window_size = 3  # Window size for moving average\n",
    "denoised_data = np.apply_along_axis(moving_average, 0, data_np, window_size)\n",
    "# Pad the denoised data to match the original length\n",
    "denoised_data = np.pad(denoised_data, ((window_size-1, 0), (0, 0)), mode='edge')\n",
    "\n",
    "# Add lag features\n",
    "max_lag = 3  # Maximum lag for lag features\n",
    "lagged_data = add_lag_features(denoised_data.reshape(1, -1, input_size), max_lag)\n",
    "lagged_data = lagged_data.reshape(-1, lagged_data.shape[2])\n",
    "\n",
    "# Prepare data for PyTorch\n",
    "seq_len = 20\n",
    "X = []\n",
    "y = []\n",
    "for i in range(len(lagged_data) - seq_len):\n",
    "    X.append(lagged_data[i:i+seq_len])\n",
    "    y.append(data_np[i+seq_len])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Initialize model\n",
    "hidden_size = 128\n",
    "output_size = input_size\n",
    "num_layers = 2\n",
    "dropout = 0.5\n",
    "model = HybridModel(input_size * (max_lag + 1), hidden_size, output_size, num_layers, dropout)\n",
    "\n",
    "# Train model\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "n_epochs = 100\n",
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "for epoch in trange(n_epochs, desc=\"Epochs\"):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        inputs, targets = batch\n",
    "        optimizer.zero_grad()\n",
    "        outputs, _ = model(inputs, targets, teacher_forcing_ratio)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    total_loss /= len(train_loader)\n",
    "    print(f'Epoch {epoch+1}/{n_epochs}, Loss: {total_loss:.4f}')\n",
    "\n",
    "# Predict\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_input = X_tensor[:1]\n",
    "    prediction, _ = model(test_input)\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from tqdm import trange\n",
    "\n",
    "# Define Attention Mechanism\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.attn.weight)\n",
    "        nn.init.constant_(self.attn.bias, 0)\n",
    "        nn.init.uniform_(self.v, -0.1, 0.1)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        seq_len = encoder_outputs.size(1)\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, seq_len, 1)\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "        energy = energy.transpose(1, 2)\n",
    "        v = self.v.repeat(encoder_outputs.size(0), 1).unsqueeze(1)\n",
    "        attention_weights = torch.bmm(v, energy).squeeze(1)\n",
    "        return torch.softmax(attention_weights, dim=1)\n",
    "\n",
    "# Define LSTM Encoder\n",
    "class LSTMEncoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout):\n",
    "        super(LSTMEncoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs, (hidden, cell) = self.lstm(x)\n",
    "        outputs = self.dropout(outputs)\n",
    "        return outputs, hidden, cell\n",
    "\n",
    "# Define LSTM Decoder with Attention\n",
    "class LSTMDecoderWithAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, num_layers, dropout):\n",
    "        super(LSTMDecoderWithAttention, self).__init__()\n",
    "        self.attention = Attention(hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size * 2, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, encoder_outputs, hidden, cell, targets=None, teacher_forcing_ratio=0.5):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        seq_len = encoder_outputs.size(1)\n",
    "        output_size = self.fc.out_features\n",
    "\n",
    "        outputs = torch.zeros(batch_size, seq_len, output_size).to(encoder_outputs.device)\n",
    "        input = encoder_outputs[:, -1, :].unsqueeze(1)  # First input to the decoder is the last hidden state of the encoder\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            attn_weights = self.attention(hidden[-1], encoder_outputs)\n",
    "            context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n",
    "            lstm_input = torch.cat([context, input.squeeze(1)], dim=1).unsqueeze(1)  # Add sequence dimension\n",
    "            output, (hidden, cell) = self.lstm(lstm_input, (hidden, cell))\n",
    "            output = self.fc(self.dropout(output.squeeze(1)))\n",
    "            outputs[:, t, :] = output\n",
    "\n",
    "            teacher_force = targets is not None and torch.rand(1).item() < teacher_forcing_ratio\n",
    "            input = targets[:, t].unsqueeze(1) if teacher_force else output.unsqueeze(1)\n",
    "\n",
    "        return outputs, attn_weights\n",
    "\n",
    "# Define the Hybrid Model\n",
    "class HybridModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, dropout):\n",
    "        super(HybridModel, self).__init__()\n",
    "        self.encoder = LSTMEncoder(input_size, hidden_size, num_layers, dropout)\n",
    "        self.decoder = LSTMDecoderWithAttention(hidden_size, output_size, num_layers, dropout)\n",
    "\n",
    "    def forward(self, past_data, targets=None, teacher_forcing_ratio=0.5):\n",
    "        encoder_outputs, hidden, cell = self.encoder(past_data)\n",
    "        prediction, attn_weights = self.decoder(encoder_outputs, hidden, cell, targets, teacher_forcing_ratio)\n",
    "        return prediction, attn_weights\n",
    "\n",
    "def moving_average(data, window_size):\n",
    "    return np.convolve(data, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "def add_lag_features(data, max_lag):\n",
    "    lagged_data = np.zeros((data.shape[0], data.shape[1], data.shape[2] * (max_lag + 1)))\n",
    "    for lag in range(max_lag + 1):\n",
    "        lagged_data[:, lag:, lag * data.shape[2]:(lag + 1) * data.shape[2]] = data[:, :data.shape[1] - lag, :]\n",
    "    return lagged_data\n",
    "\n",
    "# Read data from CSV\n",
    "data = pd.read_csv('time_series_data.csv', index_col='date', parse_dates=True)\n",
    "features = data.columns.tolist()\n",
    "input_size = len(features)\n",
    "\n",
    "# Convert to numpy array\n",
    "data_np = data.values\n",
    "\n",
    "# Denoise data using moving average\n",
    "window_size = 3  # Window size for moving average\n",
    "denoised_data = np.apply_along_axis(moving_average, 0, data_np, window_size)\n",
    "# Pad the denoised data to match the original length\n",
    "denoised_data = np.pad(denoised_data, ((window_size-1, 0), (0, 0)), mode='edge')\n",
    "\n",
    "# Add lag features\n",
    "max_lag = 3  # Maximum lag for lag features\n",
    "lagged_data = add_lag_features(denoised_data.reshape(1, -1, input_size), max_lag)\n",
    "lagged_data = lagged_data.reshape(-1, lagged_data.shape[2])\n",
    "\n",
    "# Prepare data for PyTorch\n",
    "seq_len = 20\n",
    "X = []\n",
    "y = []\n",
    "for i in range(len(lagged_data) - seq_len):\n",
    "    X.append(lagged_data[i:i+seq_len])\n",
    "    y.append(data_np[i+seq_len])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Initialize model\n",
    "hidden_size = 128\n",
    "output_size = input_size\n",
    "num_layers = 2\n",
    "dropout = 0.5\n",
    "\n",
    "# Bagging\n",
    "num_models = 5\n",
    "models = [HybridModel(input_size * (max_lag + 1), hidden_size, output_size, num_layers, dropout) for _ in range(num_models)]\n",
    "criterions = [nn.MSELoss() for _ in range(num_models)]\n",
    "optimizers = [optim.Adam(model.parameters(), lr=0.001) for model in models]\n",
    "\n",
    "# Train models\n",
    "n_epochs = 100\n",
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "for epoch in trange(n_epochs, desc=\"Epochs\"):\n",
    "    for model, optimizer, criterion in zip(models, optimizers, criterions):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            inputs, targets = batch\n",
    "            optimizer.zero_grad()\n",
    "            outputs, _ = model(inputs, targets, teacher_forcing_ratio)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        total_loss /= len(train_loader)\n",
    "        print(f'Epoch {epoch+1}/{n_epochs}, Loss: {total_loss:.4f}')\n",
    "\n",
    "# Predict with bagging\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_input = X_tensor[:1]\n",
    "    predictions = [model(test_input)[0] for model in models]\n",
    "    avg_prediction = torch.mean(torch.stack(predictions), dim=0)\n",
    "    print(avg_prediction)\n",
    "\n",
    "# Boosting (simple example with sequential training)\n",
    "boosted_model = HybridModel(input_size * (max_lag + 1), hidden_size, output_size, num_layers, dropout)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(boosted_model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in trange(n_epochs, desc=\"Epochs\"):\n",
    "    boosted_model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        inputs, targets = batch\n",
    "        optimizer.zero_grad()\n",
    "        outputs, _ = boosted_model(inputs, targets, teacher_forcing_ratio)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    total_loss /= len(train_loader)\n",
    "    print(f'Epoch {epoch+1}/{n_epochs}, Loss: {total_loss:.4f}')\n",
    "\n",
    "# Predict with boosting\n",
    "boosted_model.eval()\n",
    "with torch.no_grad():\n",
    "    test_input = X_tensor[:1]\n",
    "    prediction, _ = boosted_model(test_input)\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
