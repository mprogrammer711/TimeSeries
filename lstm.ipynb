{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMDecoderWithTeacherForcing(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, dropout):\n",
    "        super(LSTMDecoderWithTeacherForcing, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    def forward(self, encoder_outputs, hidden, cell, targets=None, teacher_forcing_ratio=0.5):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        seq_len = encoder_outputs.size(1)\n",
    "        output_size = self.fc.out_features\n",
    "\n",
    "        outputs = torch.zeros(batch_size, seq_len, output_size).to(encoder_outputs.device)\n",
    "        input = encoder_outputs[:, 0, :].unsqueeze(1)  # First input to the decoder is the first encoder output\n",
    "\n",
    "        for t in range(1, seq_len):\n",
    "            output, (hidden, cell) = self.lstm(input, (hidden, cell))\n",
    "            output = self.fc(output.squeeze(1))\n",
    "            outputs[:, t, :] = output\n",
    "\n",
    "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
    "            input = targets[:, t, :].unsqueeze(1) if teacher_force else output.unsqueeze(1)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "# Example usage\n",
    "input_size = 10\n",
    "hidden_size = 20\n",
    "output_size = 10\n",
    "num_layers = 2\n",
    "dropout = 0.5\n",
    "\n",
    "decoder = LSTMDecoderWithTeacherForcing(input_size, hidden_size, output_size, num_layers, dropout)\n",
    "print(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMDecoderWithAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, num_layers, dropout, context_size):\n",
    "        super(LSTMDecoderWithAttention, self).__init__()\n",
    "        self.attention = Attention(hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size + context_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, encoder_outputs, hidden, cell, current_context, targets=None, teacher_forcing_ratio=0.5):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        seq_len = encoder_outputs.size(1)\n",
    "        output_size = self.fc.out_features\n",
    "\n",
    "        outputs = torch.zeros(batch_size, seq_len, output_size).to(encoder_outputs.device)\n",
    "        input = current_context.unsqueeze(1)  # First input to the decoder is the current context\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            attn_weights = self.attention(hidden[-1], encoder_outputs)\n",
    "            context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n",
    "            lstm_input = torch.cat([context, input.squeeze(1)], dim=1).unsqueeze(1)  # Add sequence dimension\n",
    "            output, (hidden, cell) = self.lstm(lstm_input, (hidden, cell))\n",
    "            output = self.fc(output.squeeze(1))\n",
    "            outputs[:, t, :] = output\n",
    "\n",
    "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
    "            input = targets[:, t, :].unsqueeze(1) if teacher_force else output.unsqueeze(1)\n",
    "\n",
    "        return outputs, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define Attention Mechanism\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.attn.weight)\n",
    "        nn.init.constant_(self.attn.bias, 0)\n",
    "        nn.init.uniform_(self.v, -0.1, 0.1)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        seq_len = encoder_outputs.size(1)\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, seq_len, 1)\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "        energy = energy.transpose(1, 2)\n",
    "        v = self.v.repeat(encoder_outputs.size(0), 1).unsqueeze(1)\n",
    "        attention_weights = torch.bmm(v, energy).squeeze(1)\n",
    "        return torch.softmax(attention_weights, dim=1)\n",
    "\n",
    "# Define LSTM Encoder\n",
    "class LSTMEncoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout):\n",
    "        super(LSTMEncoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs, (hidden, cell) = self.lstm(x)\n",
    "        return outputs, hidden, cell\n",
    "\n",
    "# Define LSTM Decoder with Attention\n",
    "class LSTMDecoderWithAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, num_layers, dropout):\n",
    "        super(LSTMDecoderWithAttention, self).__init__()\n",
    "        self.attention = Attention(hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size * 2, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, encoder_outputs, hidden, cell, targets=None, teacher_forcing_ratio=0.5):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        seq_len = encoder_outputs.size(1)\n",
    "        output_size = self.fc.out_features\n",
    "\n",
    "        outputs = torch.zeros(batch_size, seq_len, output_size).to(encoder_outputs.device)\n",
    "        input = encoder_outputs[:, -1, :].unsqueeze(1)  # First input to the decoder is the last hidden state of the encoder\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            attn_weights = self.attention(hidden[-1], encoder_outputs)\n",
    "            context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n",
    "            lstm_input = torch.cat([context, input.squeeze(1)], dim=1).unsqueeze(1)  # Add sequence dimension\n",
    "            output, (hidden, cell) = self.lstm(lstm_input, (hidden, cell))\n",
    "            output = self.fc(output.squeeze(1))\n",
    "            outputs[:, t, :] = output\n",
    "\n",
    "            teacher_force = targets is not None and torch.rand(1).item() < teacher_forcing_ratio\n",
    "            input = targets[:, t].unsqueeze(1) if teacher_force else output.unsqueeze(1)\n",
    "\n",
    "        return outputs, attn_weights\n",
    "\n",
    "# Define the Hybrid Model\n",
    "class HybridModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, dropout):\n",
    "        super(HybridModel, self).__init__()\n",
    "        self.encoder = LSTMEncoder(input_size, hidden_size, num_layers, dropout)\n",
    "        self.decoder = LSTMDecoderWithAttention(hidden_size, output_size, num_layers, dropout)\n",
    "\n",
    "    def forward(self, past_data, targets=None, teacher_forcing_ratio=0.5):\n",
    "        encoder_outputs, hidden, cell = self.encoder(past_data)\n",
    "        prediction, attn_weights = self.decoder(encoder_outputs, hidden, cell, targets, teacher_forcing_ratio)\n",
    "        return prediction, attn_weights\n",
    "\n",
    "# Example usage\n",
    "input_size = 10\n",
    "hidden_size = 128\n",
    "output_size = 1\n",
    "num_layers = 2\n",
    "dropout = 0.5\n",
    "batch_size = 32\n",
    "seq_len = 20\n",
    "\n",
    "model = HybridModel(input_size, hidden_size, output_size, num_layers, dropout)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Dummy data\n",
    "past_data = torch.randn(batch_size, seq_len, input_size)\n",
    "targets = torch.randn(batch_size, seq_len, output_size)\n",
    "\n",
    "# Forward pass\n",
    "outputs, attn_weights = model(past_data, targets, teacher_forcing_ratio=0.5)\n",
    "loss = criterion(outputs, targets)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print(\"Output shape:\", outputs.shape)\n",
    "print(\"Attention weights shape:\", attn_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TimeSeriesTransformer(nn.Module):\n",
    "    def __init__(self, config, input_size, output_size):\n",
    "        super(TimeSeriesTransformer, self).__init__()\n",
    "        self.config = config\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Embedding layer for continuous inputs\n",
    "        self.input_embedding = nn.Linear(input_size, config.n_embd)\n",
    "        \n",
    "        # Positional encodings\n",
    "        self.position_embeddings = nn.Parameter(torch.zeros(1, config.block_size, config.n_embd))\n",
    "        \n",
    "        # Transformer layers\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=config.n_embd, nhead=config.n_head, num_encoder_layers=config.n_layer\n",
    "        )\n",
    "        \n",
    "        # Output layer to map transformer output to target size\n",
    "        self.output_layer = nn.Linear(config.n_embd, output_size)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # Embed the inputs and add positional encodings\n",
    "        x = self.input_embedding(inputs) + self.position_embeddings[:, :inputs.size(1), :]\n",
    "        \n",
    "        # Transformer encoding\n",
    "        x = self.transformer(x)\n",
    "        \n",
    "        # Output prediction\n",
    "        out = self.output_layer(x)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Configuration and model instantiation\n",
    "class GPTConfig:\n",
    "    n_embd = 128  # Embedding size\n",
    "    n_head = 8    # Number of heads in multi-head attention\n",
    "    n_layer = 4   # Number of transformer layers\n",
    "    block_size = 30  # Maximum sequence length (e.g., number of days)\n",
    "\n",
    "input_size = 5  # Number of features per day (e.g., target + macro data)\n",
    "output_size = 1  # Number of target values per day (e.g., forecasting a single target)\n",
    "config = GPTConfig()\n",
    "\n",
    "model = TimeSeriesTransformer(config, input_size=input_size, output_size=output_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def train_lstm_with_teacher_forcing(model, train_loader, criterion, optimizer, num_epochs, teacher_forcing_ratio=0.5):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in train_loader:\n",
    "            inputs, targets = batch\n",
    "            inputs, targets = inputs.to(model.device), targets.to(model.device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs, targets, teacher_forcing_ratio=teacher_forcing_ratio)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Example usage\n",
    "batch_size = 32\n",
    "seq_len = 10\n",
    "input_size = 10\n",
    "\n",
    "# Create a random dataset\n",
    "train_data = torch.randn(batch_size, seq_len, input_size)\n",
    "train_targets = torch.randn(batch_size, seq_len, input_size)\n",
    "train_dataset = torch.utils.data.TensorDataset(train_data, train_targets)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize model, criterion, and optimizer\n",
    "model = LSTMDecoderWithTeacherForcing(input_size, hidden_size, output_size, num_layers, dropout)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model with teacher forcing\n",
    "train_lstm_with_teacher_forcing(model, train_loader, criterion, optimizer, num_epochs=10, teacher_forcing_ratio=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
