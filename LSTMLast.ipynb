{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define LSTM Encoder\n",
    "class LSTMEncoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout):\n",
    "        super(LSTMEncoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs, (hidden, cell) = self.lstm(x)\n",
    "        return outputs, hidden, cell\n",
    "\n",
    "# Define LSTM Decoder\n",
    "class LSTMDecoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, num_layers, dropout):\n",
    "        super(LSTMDecoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, encoder_outputs, hidden, cell, seq_len):\n",
    "        batch_size = hidden.size(1)\n",
    "        output_size = self.fc.out_features\n",
    "\n",
    "        outputs = torch.zeros(batch_size, seq_len, output_size).to(hidden.device)\n",
    "        input = encoder_outputs[:, -1, :].unsqueeze(1)  # First input to the decoder is the last hidden state of the encoder\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            output, (hidden, cell) = self.lstm(input, (hidden, cell))\n",
    "            output = self.fc(output.squeeze(1))\n",
    "            outputs[:, t, :] = output\n",
    "            input = output.unsqueeze(1)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "# Define the Hybrid Model\n",
    "class HybridModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, dropout):\n",
    "        super(HybridModel, self).__init__()\n",
    "        self.encoder = LSTMEncoder(input_size, hidden_size, num_layers, dropout)\n",
    "        self.decoder = LSTMDecoder(hidden_size, output_size, num_layers, dropout)\n",
    "\n",
    "    def forward(self, past_data, seq_len):\n",
    "        encoder_outputs, hidden, cell = self.encoder(past_data)\n",
    "        prediction = self.decoder(encoder_outputs, hidden, cell, seq_len)\n",
    "        return prediction\n",
    "\n",
    "# Example usage\n",
    "input_size = 10\n",
    "hidden_size = 128\n",
    "output_size = 1\n",
    "num_layers = 2\n",
    "dropout = 0.5\n",
    "batch_size = 32\n",
    "seq_len = 20\n",
    "\n",
    "model = HybridModel(input_size, hidden_size, output_size, num_layers, dropout)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Dummy data\n",
    "past_data = torch.randn(batch_size, seq_len, input_size)\n",
    "targets = torch.randn(batch_size, seq_len, output_size)\n",
    "\n",
    "# Forward pass\n",
    "outputs = model(past_data, seq_len)\n",
    "loss = criterion(outputs, targets)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print(\"Output shape:\", outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define Attention Mechanism\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.attn.weight)\n",
    "        nn.init.constant_(self.attn.bias, 0)\n",
    "        nn.init.uniform_(self.v, -0.1, 0.1)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        seq_len = encoder_outputs.size(1)\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, seq_len, 1)\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "        energy = energy.transpose(1, 2)\n",
    "        v = self.v.repeat(encoder_outputs.size(0), 1).unsqueeze(1)\n",
    "        attention_weights = torch.bmm(v, energy).squeeze(1)\n",
    "        return torch.softmax(attention_weights, dim=1)\n",
    "\n",
    "# Define LSTM Encoder\n",
    "class LSTMEncoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout):\n",
    "        super(LSTMEncoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs, (hidden, cell) = self.lstm(x)\n",
    "        return outputs, hidden, cell\n",
    "\n",
    "# Define LSTM Decoder with Attention\n",
    "class LSTMDecoderWithAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, num_layers, dropout):\n",
    "        super(LSTMDecoderWithAttention, self).__init__()\n",
    "        self.attention = Attention(hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size * 2, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, encoder_outputs, hidden, cell, seq_len):\n",
    "        batch_size = hidden.size(1)\n",
    "        output_size = self.fc.out_features\n",
    "\n",
    "        outputs = torch.zeros(batch_size, seq_len, output_size).to(hidden.device)\n",
    "        input = encoder_outputs[:, -1, :].unsqueeze(1)  # First input to the decoder is the last hidden state of the encoder\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            attn_weights = self.attention(hidden[-1], encoder_outputs)\n",
    "            context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n",
    "            lstm_input = torch.cat([context, input.squeeze(1)], dim=1).unsqueeze(1)  # Add sequence dimension\n",
    "            output, (hidden, cell) = self.lstm(lstm_input, (hidden, cell))\n",
    "            output = self.fc(output.squeeze(1))\n",
    "            outputs[:, t, :] = output\n",
    "            input = output.unsqueeze(1)\n",
    "\n",
    "        return outputs, attn_weights\n",
    "\n",
    "# Define the Hybrid Model\n",
    "class HybridModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, dropout):\n",
    "        super(HybridModel, self).__init__()\n",
    "        self.encoder = LSTMEncoder(input_size, hidden_size, num_layers, dropout)\n",
    "        self.decoder = LSTMDecoderWithAttention(hidden_size, output_size, num_layers, dropout)\n",
    "\n",
    "    def forward(self, past_data, seq_len):\n",
    "        encoder_outputs, hidden, cell = self.encoder(past_data)\n",
    "        prediction, attn_weights = self.decoder(encoder_outputs, hidden, cell, seq_len)\n",
    "        return prediction, attn_weights\n",
    "\n",
    "# Example usage\n",
    "input_size = 10\n",
    "hidden_size = 128\n",
    "output_size = 1\n",
    "num_layers = 2\n",
    "dropout = 0.5\n",
    "batch_size = 32\n",
    "seq_len = 20\n",
    "\n",
    "model = HybridModel(input_size, hidden_size, output_size, num_layers, dropout)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Dummy data\n",
    "past_data = torch.randn(batch_size, seq_len, input_size)\n",
    "targets = torch.randn(batch_size, seq_len, output_size)\n",
    "\n",
    "# Forward pass\n",
    "outputs, attn_weights = model(past_data, seq_len)\n",
    "loss = criterion(outputs, targets)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print(\"Output shape:\", outputs.shape)\n",
    "print(\"Attention weights shape:\", attn_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define Attention Mechanism\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.attn.weight)\n",
    "        nn.init.constant_(self.attn.bias, 0)\n",
    "        nn.init.uniform_(self.v, -0.1, 0.1)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        seq_len = encoder_outputs.size(1)\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, seq_len, 1)\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "        energy = energy.transpose(1, 2)\n",
    "        v = self.v.repeat(encoder_outputs.size(0), 1).unsqueeze(1)\n",
    "        attention_weights = torch.bmm(v, energy).squeeze(1)\n",
    "        return torch.softmax(attention_weights, dim=1)\n",
    "\n",
    "# Define LSTM Encoder\n",
    "class LSTMEncoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout):\n",
    "        super(LSTMEncoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs, (hidden, cell) = self.lstm(x)\n",
    "        return outputs, hidden, cell\n",
    "\n",
    "# Define LSTM Decoder with Attention and Teacher Forcing\n",
    "class LSTMDecoderWithAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, num_layers, dropout):\n",
    "        super(LSTMDecoderWithAttention, self).__init__()\n",
    "        self.attention = Attention(hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size * 2, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, encoder_outputs, hidden, cell, targets=None, teacher_forcing_ratio=0.5):\n",
    "        batch_size = hidden.size(1)\n",
    "        seq_len = targets.size(1) if targets is not None else encoder_outputs.size(1)\n",
    "        output_size = self.fc.out_features\n",
    "\n",
    "        outputs = torch.zeros(batch_size, seq_len, output_size).to(hidden.device)\n",
    "        input = encoder_outputs[:, -1, :].unsqueeze(1)  # First input to the decoder is the last hidden state of the encoder\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            attn_weights = self.attention(hidden[-1], encoder_outputs)\n",
    "            context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n",
    "            lstm_input = torch.cat([context, input.squeeze(1)], dim=1).unsqueeze(1)  # Add sequence dimension\n",
    "            output, (hidden, cell) = self.lstm(lstm_input, (hidden, cell))\n",
    "            output = self.fc(output.squeeze(1))\n",
    "            outputs[:, t, :] = output\n",
    "\n",
    "            teacher_force = targets is not None and torch.rand(1).item() < teacher_forcing_ratio\n",
    "            input = targets[:, t].unsqueeze(1) if teacher_force else output.unsqueeze(1)\n",
    "\n",
    "        return outputs, attn_weights\n",
    "\n",
    "# Define the Hybrid Model\n",
    "class HybridModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, dropout):\n",
    "        super(HybridModel, self).__init__()\n",
    "        self.encoder = LSTMEncoder(input_size, hidden_size, num_layers, dropout)\n",
    "        self.decoder = LSTMDecoderWithAttention(hidden_size, output_size, num_layers, dropout)\n",
    "\n",
    "    def forward(self, past_data, targets=None, teacher_forcing_ratio=0.5):\n",
    "        encoder_outputs, hidden, cell = self.encoder(past_data)\n",
    "        prediction, attn_weights = self.decoder(encoder_outputs, hidden, cell, targets, teacher_forcing_ratio)\n",
    "        return prediction, attn_weights\n",
    "\n",
    "# Example usage\n",
    "input_size = 10\n",
    "hidden_size = 128\n",
    "output_size = 1\n",
    "num_layers = 2\n",
    "dropout = 0.5\n",
    "batch_size = 32\n",
    "seq_len = 20\n",
    "\n",
    "model = HybridModel(input_size, hidden_size, output_size, num_layers, dropout)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Dummy data\n",
    "past_data = torch.randn(batch_size, seq_len, input_size)\n",
    "targets = torch.randn(batch_size, seq_len, output_size)\n",
    "\n",
    "# Forward pass\n",
    "outputs, attn_weights = model(past_data, targets, teacher_forcing_ratio=0.5)\n",
    "loss = criterion(outputs, targets)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print(\"Output shape:\", outputs.shape)\n",
    "print(\"Attention weights shape:\", attn_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define Attention Mechanism\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.attn.weight)\n",
    "        nn.init.constant_(self.attn.bias, 0)\n",
    "        nn.init.uniform_(self.v, -0.1, 0.1)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        seq_len = encoder_outputs.size(1)\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, seq_len, 1)\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "        energy = energy.transpose(1, 2)\n",
    "        v = self.v.repeat(encoder_outputs.size(0), 1).unsqueeze(1)\n",
    "        attention_weights = torch.bmm(v, energy).squeeze(1)\n",
    "        return torch.softmax(attention_weights, dim=1)\n",
    "\n",
    "# Define LSTM Encoder\n",
    "class LSTMEncoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout):\n",
    "        super(LSTMEncoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs, (hidden, cell) = self.lstm(x)\n",
    "        return outputs, hidden, cell\n",
    "\n",
    "# Define LSTM Decoder with Attention and Teacher Forcing\n",
    "class LSTMDecoderWithAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, num_layers, dropout, macro_size):\n",
    "        super(LSTMDecoderWithAttention, self).__init__()\n",
    "        self.attention = Attention(hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size * 2 + macro_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, encoder_outputs, hidden, cell, macro_data, targets=None, teacher_forcing_ratio=0.5):\n",
    "        batch_size = hidden.size(1)\n",
    "        seq_len = targets.size(1) if targets is not None else encoder_outputs.size(1)\n",
    "        output_size = self.fc.out_features\n",
    "\n",
    "        outputs = torch.zeros(batch_size, seq_len, output_size).to(hidden.device)\n",
    "        input = torch.cat([encoder_outputs[:, -1, :], macro_data], dim=1).unsqueeze(1)  # First input to the decoder is the last hidden state of the encoder concatenated with macro data\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            attn_weights = self.attention(hidden[-1], encoder_outputs)\n",
    "            context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n",
    "            lstm_input = torch.cat([context, input.squeeze(1), macro_data], dim=1).unsqueeze(1)  # Add sequence dimension\n",
    "            output, (hidden, cell) = self.lstm(lstm_input, (hidden, cell))\n",
    "            output = self.fc(output.squeeze(1))\n",
    "            outputs[:, t, :] = output\n",
    "\n",
    "            teacher_force = targets is not None and torch.rand(1).item() < teacher_forcing_ratio\n",
    "            input = torch.cat([targets[:, t], macro_data], dim=1).unsqueeze(1) if teacher_force else torch.cat([output, macro_data], dim=1).unsqueeze(1)\n",
    "\n",
    "        return outputs, attn_weights\n",
    "\n",
    "# Define the Hybrid Model\n",
    "class HybridModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, dropout, macro_size):\n",
    "        super(HybridModel, self).__init__()\n",
    "        self.encoder = LSTMEncoder(input_size, hidden_size, num_layers, dropout)\n",
    "        self.decoder = LSTMDecoderWithAttention(hidden_size, output_size, num_layers, dropout, macro_size)\n",
    "\n",
    "    def forward(self, past_data, macro_data, targets=None, teacher_forcing_ratio=0.5):\n",
    "        encoder_outputs, hidden, cell = self.encoder(past_data)\n",
    "        prediction, attn_weights = self.decoder(encoder_outputs, hidden, cell, macro_data, targets, teacher_forcing_ratio)\n",
    "        return prediction, attn_weights\n",
    "\n",
    "# Example usage\n",
    "input_size = 10\n",
    "hidden_size = 128\n",
    "output_size = 1\n",
    "num_layers = 2\n",
    "dropout = 0.5\n",
    "batch_size = 32\n",
    "seq_len = 20\n",
    "macro_size = 5  # Size of today's macro data\n",
    "\n",
    "model = HybridModel(input_size, hidden_size, output_size, num_layers, dropout, macro_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Dummy data\n",
    "past_data = torch.randn(batch_size, seq_len, input_size)\n",
    "macro_data = torch.randn(batch_size, macro_size)\n",
    "targets = torch.randn(batch_size, seq_len, output_size)\n",
    "\n",
    "# Forward pass\n",
    "outputs, attn_weights = model(past_data, macro_data, targets, teacher_forcing_ratio=0.5)\n",
    "loss = criterion(outputs, targets)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print(\"Output shape:\", outputs.shape)\n",
    "print(\"Attention weights shape:\", attn_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define Convolutional Layer\n",
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, kernel_size, dropout):\n",
    "        super(ConvLayer, self).__init__()\n",
    "        self.conv = nn.Conv1d(input_size, hidden_size, kernel_size, padding=(kernel_size - 1) // 2)\n",
    "        self.bn = nn.BatchNorm1d(hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)  # Change shape to (batch_size, input_size, seq_len)\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x.permute(0, 2, 1)  # Change shape back to (batch_size, seq_len, hidden_size)\n",
    "        return x\n",
    "\n",
    "# Define Attention Mechanism\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.attn.weight)\n",
    "        nn.init.constant_(self.attn.bias, 0)\n",
    "        nn.init.uniform_(self.v, -0.1, 0.1)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        seq_len = encoder_outputs.size(1)\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, seq_len, 1)\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "        energy = energy.transpose(1, 2)\n",
    "        v = self.v.repeat(encoder_outputs.size(0), 1).unsqueeze(1)\n",
    "        attention_weights = torch.bmm(v, energy).squeeze(1)\n",
    "        return torch.softmax(attention_weights, dim=1)\n",
    "\n",
    "# Define LSTM Encoder\n",
    "class LSTMEncoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout):\n",
    "        super(LSTMEncoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs, (hidden, cell) = self.lstm(x)\n",
    "        return outputs, hidden, cell\n",
    "\n",
    "# Define Transformer Layer\n",
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads, num_layers, dropout):\n",
    "        super(TransformerLayer, self).__init__()\n",
    "        self.transformer = nn.Transformer(hidden_size, num_heads, num_layers, dropout=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(1, 0, 2)  # Change shape to (seq_len, batch_size, hidden_size)\n",
    "        x = self.transformer(x, x)\n",
    "        x = x.permute(1, 0, 2)  # Change shape back to (batch_size, seq_len, hidden_size)\n",
    "        return x\n",
    "\n",
    "# Define LSTM Decoder with Attention and Teacher Forcing\n",
    "class LSTMDecoderWithAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, num_layers, dropout, macro_size):\n",
    "        super(LSTMDecoderWithAttention, self).__init__()\n",
    "        self.attention = Attention(hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size * 2 + macro_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, encoder_outputs, hidden, cell, macro_data, targets=None, teacher_forcing_ratio=0.5):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        seq_len = encoder_outputs.size(1)\n",
    "        output_size = self.fc.out_features\n",
    "\n",
    "        outputs = torch.zeros(batch_size, seq_len, output_size).to(encoder_outputs.device)\n",
    "        input = encoder_outputs[:, -1, :].unsqueeze(1)  # First input to the decoder is the last hidden state of the encoder\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            attn_weights = self.attention(hidden[-1], encoder_outputs)\n",
    "            context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n",
    "            lstm_input = torch.cat([context, input.squeeze(1), macro_data], dim=1).unsqueeze(1)  # Add sequence dimension\n",
    "            output, (hidden, cell) = self.lstm(lstm_input, (hidden, cell))\n",
    "            output = self.fc(output.squeeze(1))\n",
    "            outputs[:, t, :] = output\n",
    "\n",
    "            teacher_force = targets is not None and torch.rand(1).item() < teacher_forcing_ratio\n",
    "            input = targets[:, t].unsqueeze(1) if teacher_force else output.unsqueeze(1)\n",
    "\n",
    "        return outputs, attn_weights\n",
    "\n",
    "# Define the Hybrid Model\n",
    "class HybridModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, dropout, macro_size, kernel_size, num_heads, num_transformer_layers):\n",
    "        super(HybridModel, self).__init__()\n",
    "        self.conv = ConvLayer(input_size, hidden_size, kernel_size, dropout)\n",
    "        self.encoder = LSTMEncoder(hidden_size, hidden_size, num_layers, dropout)\n",
    "        self.decoder = LSTMDecoderWithAttention(hidden_size, output_size, num_layers, dropout, macro_size)\n",
    "        self.transformer = TransformerLayer(hidden_size, num_heads, num_transformer_layers, dropout)\n",
    "\n",
    "    def forward(self, past_data, macro_data, targets=None, teacher_forcing_ratio=0.5):\n",
    "        conv_output = self.conv(past_data)\n",
    "        encoder_outputs, hidden, cell = self.encoder(conv_output)\n",
    "        prediction, attn_weights = self.decoder(encoder_outputs, hidden, cell, macro_data, targets, teacher_forcing_ratio)\n",
    "        transformer_output = self.transformer(prediction)\n",
    "        return transformer_output, attn_weights\n",
    "\n",
    "# Example usage\n",
    "input_size = 10\n",
    "hidden_size = 128\n",
    "output_size = 1\n",
    "num_layers = 2\n",
    "dropout = 0.5\n",
    "batch_size = 32\n",
    "seq_len = 20\n",
    "macro_size = 5  # Size of today's macro data\n",
    "kernel_size = 3\n",
    "num_heads = 8\n",
    "num_transformer_layers = 2\n",
    "\n",
    "model = HybridModel(input_size, hidden_size, output_size, num_layers, dropout, macro_size, kernel_size, num_heads, num_transformer_layers)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Dummy data\n",
    "past_data = torch.randn(batch_size, seq_len, input_size)\n",
    "macro_data = torch.randn(batch_size, macro_size)\n",
    "targets = torch.randn(batch_size, seq_len, output_size)\n",
    "\n",
    "# Forward pass\n",
    "outputs, attn_weights = model(past_data, macro_data, targets, teacher_forcing_ratio=0.5)\n",
    "loss = criterion(outputs, targets)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print(\"Output shape:\", outputs.shape)\n",
    "print(\"Attention weights shape:\", attn_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
